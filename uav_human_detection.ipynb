{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš Robust Human Detection in UAV Imagery\n",
    "## Using HIT-UAV Infrared Thermal Dataset\n",
    "\n",
    "**Features:**\n",
    "- HIT-UAV thermal dataset ingestion with COCO format conversion\n",
    "- Scene filtering to focus on SAR-relevant imagery\n",
    "- Realistic augmentations: snow, smoke/fire, thermal artifacts\n",
    "- Transfer learning with Faster R-CNN (or YOLOv8)\n",
    "- Robust evaluation on clean vs perturbed data\n",
    "\n",
    "**Runtime:** ~15-20 min on Colab GPU (T4)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Environment & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: ENVIRONMENT SETUP\n",
    "# Install packages and mount Drive for persistent storage\n",
    "# =============================================================================\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Install required packages for UAV detection pipeline.\"\"\"\n",
    "    packages = [\n",
    "        'torch',\n",
    "        'torchvision',\n",
    "        'albumentations>=1.3.0',\n",
    "        'pycocotools',\n",
    "        'opencv-python-headless',\n",
    "        'matplotlib',\n",
    "        'numpy',\n",
    "        'Pillow',\n",
    "        'tqdm',\n",
    "        'gdown',  # For Google Drive downloads\n",
    "        'scipy',\n",
    "    ]\n",
    "    \n",
    "    for pkg in packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pkg])\n",
    "        except:\n",
    "            print(f\"Warning: Could not install {pkg}\")\n",
    "    \n",
    "    print(\"âœ“ Packages installed successfully\")\n",
    "\n",
    "# Run installation\n",
    "install_packages()\n",
    "\n",
    "# Mount Google Drive (for Colab)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DRIVE_ROOT = '/content/drive/MyDrive/uav_detection'\n",
    "    IN_COLAB = True\n",
    "    print(f\"âœ“ Google Drive mounted at {DRIVE_ROOT}\")\n",
    "except ImportError:\n",
    "    # Running locally\n",
    "    DRIVE_ROOT = './uav_detection_cache'\n",
    "    IN_COLAB = False\n",
    "    print(f\"âœ“ Running locally, cache dir: {DRIVE_ROOT}\")\n",
    "\n",
    "import os\n",
    "os.makedirs(DRIVE_ROOT, exist_ok=True)\n",
    "os.makedirs(f\"{DRIVE_ROOT}/data\", exist_ok=True)\n",
    "os.makedirs(f\"{DRIVE_ROOT}/checkpoints\", exist_ok=True)\n",
    "os.makedirs(f\"{DRIVE_ROOT}/outputs\", exist_ok=True)\n",
    "\n",
    "print(f\"âœ“ Directory structure created under {DRIVE_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: IMPORTS AND CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    \"\"\"Central configuration for the pipeline.\"\"\"\n",
    "    # Paths\n",
    "    DATA_ROOT = f\"{DRIVE_ROOT}/data/hit_uav\"\n",
    "    CURATED_ROOT = f\"{DRIVE_ROOT}/data/curated\"\n",
    "    CHECKPOINT_DIR = f\"{DRIVE_ROOT}/checkpoints\"\n",
    "    OUTPUT_DIR = f\"{DRIVE_ROOT}/outputs\"\n",
    "    \n",
    "    # Image settings\n",
    "    IMG_SIZE = 512  # Resize for fast iteration\n",
    "    \n",
    "    # Training settings\n",
    "    BATCH_SIZE = 4\n",
    "    NUM_EPOCHS = 5  # Quick training for demo\n",
    "    LR = 0.001\n",
    "    MOMENTUM = 0.9\n",
    "    WEIGHT_DECAY = 0.0005\n",
    "    \n",
    "    # Detection settings\n",
    "    NUM_CLASSES = 2  # background + person\n",
    "    IOU_THRESHOLD = 0.5\n",
    "    CONF_THRESHOLD = 0.5\n",
    "    \n",
    "    # Device\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Random seed\n",
    "    SEED = 42\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(Config.SEED)\n",
    "np.random.seed(Config.SEED)\n",
    "random.seed(Config.SEED)\n",
    "\n",
    "print(f\"âœ“ Configuration loaded\")\n",
    "print(f\"  Device: {Config.DEVICE}\")\n",
    "print(f\"  Image size: {Config.IMG_SIZE}x{Config.IMG_SIZE}\")\n",
    "print(f\"  Epochs: {Config.NUM_EPOCHS}, Batch size: {Config.BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Data Ingestion - HIT-UAV Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: DATA INGESTION\n",
    "# Download HIT-UAV dataset, with fallbacks to VisDrone or synthetic data\n",
    "# =============================================================================\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import tarfile\n",
    "\n",
    "class DatasetDownloader:\n",
    "    \"\"\"Handles dataset downloading with multiple fallback options.\"\"\"\n",
    "    \n",
    "    # HIT-UAV GitHub repo contains annotations; images via Google Drive\n",
    "    HIT_UAV_GITHUB = \"https://github.com/suojiashun/HIT-UAV-Infrared-Thermal-Dataset\"\n",
    "    HIT_UAV_GDRIVE_ID = \"1hLaJNqLdoOjjnl_-v1jQzNdIuLQyMhZB\"  # Sample subset\n",
    "    \n",
    "    # Fallback: VisDrone (RGB UAV dataset)\n",
    "    VISDRONE_URL = \"https://github.com/VisDrone/VisDrone-Dataset\"\n",
    "    \n",
    "    def __init__(self, data_root: str):\n",
    "        self.data_root = Path(data_root)\n",
    "        self.data_root.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    def download_hit_uav(self) -> bool:\n",
    "        \"\"\"Attempt to download HIT-UAV dataset.\"\"\"\n",
    "        print(\"Attempting to download HIT-UAV dataset...\")\n",
    "        \n",
    "        try:\n",
    "            import gdown\n",
    "            \n",
    "            # Check if already downloaded\n",
    "            images_dir = self.data_root / \"images\"\n",
    "            if images_dir.exists() and len(list(images_dir.glob(\"*.jpg\"))) > 50:\n",
    "                print(\"âœ“ HIT-UAV already cached in Drive\")\n",
    "                return True\n",
    "            \n",
    "            # Try downloading sample subset from Google Drive\n",
    "            zip_path = self.data_root / \"hit_uav_sample.zip\"\n",
    "            \n",
    "            # Using a public sample or the main dataset\n",
    "            gdown.download(\n",
    "                f\"https://drive.google.com/uc?id={self.HIT_UAV_GDRIVE_ID}\",\n",
    "                str(zip_path),\n",
    "                quiet=False\n",
    "            )\n",
    "            \n",
    "            # Extract\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "                zf.extractall(self.data_root)\n",
    "            \n",
    "            zip_path.unlink()  # Remove zip\n",
    "            print(\"âœ“ HIT-UAV downloaded and extracted\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  HIT-UAV download failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def create_synthetic_thermal_dataset(self, num_images: int = 200) -> bool:\n",
    "        \"\"\"\n",
    "        Create synthetic thermal-like images with person annotations.\n",
    "        Fallback when real dataset unavailable.\n",
    "        \"\"\"\n",
    "        print(\"Creating synthetic thermal dataset for demo...\")\n",
    "        \n",
    "        images_dir = self.data_root / \"images\"\n",
    "        images_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        annotations = {\n",
    "            \"images\": [],\n",
    "            \"annotations\": [],\n",
    "            \"categories\": [{\"id\": 1, \"name\": \"person\", \"supercategory\": \"human\"}]\n",
    "        }\n",
    "        \n",
    "        ann_id = 1\n",
    "        \n",
    "        for img_id in tqdm(range(1, num_images + 1), desc=\"Generating images\"):\n",
    "            # Create thermal-like background (grayscale with noise)\n",
    "            h, w = Config.IMG_SIZE, Config.IMG_SIZE\n",
    "            \n",
    "            # Random background intensity (simulating different thermal scenes)\n",
    "            base_temp = np.random.randint(40, 120)\n",
    "            img = np.ones((h, w), dtype=np.uint8) * base_temp\n",
    "            \n",
    "            # Add terrain-like noise\n",
    "            noise = np.random.normal(0, 15, (h, w))\n",
    "            img = np.clip(img + noise, 0, 255).astype(np.uint8)\n",
    "            \n",
    "            # Add some structure (trees, buildings as blobs)\n",
    "            num_structures = np.random.randint(2, 6)\n",
    "            for _ in range(num_structures):\n",
    "                cx, cy = np.random.randint(50, w-50), np.random.randint(50, h-50)\n",
    "                radius = np.random.randint(20, 80)\n",
    "                temp = np.random.randint(60, 140)\n",
    "                cv2.circle(img, (cx, cy), radius, int(temp), -1)\n",
    "            \n",
    "            # Apply Gaussian blur for thermal camera effect\n",
    "            img = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "            \n",
    "            # Add 1-4 \"person\" hot spots\n",
    "            num_persons = np.random.randint(1, 5)\n",
    "            persons = []\n",
    "            \n",
    "            for _ in range(num_persons):\n",
    "                # Person size varies with simulated distance\n",
    "                pw = np.random.randint(15, 50)\n",
    "                ph = int(pw * np.random.uniform(1.8, 2.5))  # Person aspect ratio\n",
    "                \n",
    "                # Random position\n",
    "                px = np.random.randint(10, w - pw - 10)\n",
    "                py = np.random.randint(10, h - ph - 10)\n",
    "                \n",
    "                # Check overlap with existing persons\n",
    "                overlap = False\n",
    "                for (ex, ey, ew, eh) in persons:\n",
    "                    if not (px + pw < ex or px > ex + ew or py + ph < ey or py > ey + eh):\n",
    "                        overlap = True\n",
    "                        break\n",
    "                \n",
    "                if overlap:\n",
    "                    continue\n",
    "                    \n",
    "                persons.append((px, py, pw, ph))\n",
    "                \n",
    "                # Draw person as hot ellipse (brighter = warmer body)\n",
    "                body_temp = np.random.randint(180, 255)\n",
    "                cv2.ellipse(img, (px + pw//2, py + ph//2), (pw//2, ph//2), \n",
    "                           0, 0, 360, int(body_temp), -1)\n",
    "                \n",
    "                # Add head (smaller, slightly brighter)\n",
    "                head_r = pw // 4\n",
    "                cv2.circle(img, (px + pw//2, py + head_r + 2), head_r, \n",
    "                          int(min(body_temp + 20, 255)), -1)\n",
    "                \n",
    "                # Add annotation\n",
    "                annotations[\"annotations\"].append({\n",
    "                    \"id\": ann_id,\n",
    "                    \"image_id\": img_id,\n",
    "                    \"category_id\": 1,\n",
    "                    \"bbox\": [px, py, pw, ph],\n",
    "                    \"area\": pw * ph,\n",
    "                    \"iscrowd\": 0\n",
    "                })\n",
    "                ann_id += 1\n",
    "            \n",
    "            # Final blur and noise\n",
    "            img = cv2.GaussianBlur(img, (3, 3), 0)\n",
    "            sensor_noise = np.random.normal(0, 3, (h, w))\n",
    "            img = np.clip(img + sensor_noise, 0, 255).astype(np.uint8)\n",
    "            \n",
    "            # Save image\n",
    "            filename = f\"thermal_{img_id:04d}.jpg\"\n",
    "            # Convert to 3-channel for consistency\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "            cv2.imwrite(str(images_dir / filename), img_rgb)\n",
    "            \n",
    "            # Add image info\n",
    "            annotations[\"images\"].append({\n",
    "                \"id\": img_id,\n",
    "                \"file_name\": filename,\n",
    "                \"width\": w,\n",
    "                \"height\": h\n",
    "            })\n",
    "        \n",
    "        # Save annotations\n",
    "        ann_path = self.data_root / \"annotations.json\"\n",
    "        with open(ann_path, 'w') as f:\n",
    "            json.dump(annotations, f)\n",
    "        \n",
    "        print(f\"âœ“ Created {num_images} synthetic thermal images\")\n",
    "        print(f\"  Total person annotations: {ann_id - 1}\")\n",
    "        return True\n",
    "    \n",
    "    def load_or_download(self) -> Tuple[Path, Path]:\n",
    "        \"\"\"Main entry: load dataset or download with fallbacks.\"\"\"\n",
    "        \n",
    "        images_dir = self.data_root / \"images\"\n",
    "        ann_path = self.data_root / \"annotations.json\"\n",
    "        \n",
    "        # Check cache first\n",
    "        if images_dir.exists() and ann_path.exists():\n",
    "            num_images = len(list(images_dir.glob(\"*.jpg\")))\n",
    "            if num_images >= 50:\n",
    "                print(f\"âœ“ Using cached dataset ({num_images} images)\")\n",
    "                return images_dir, ann_path\n",
    "        \n",
    "        # Try downloading HIT-UAV\n",
    "        if self.download_hit_uav():\n",
    "            # Convert to COCO format if needed\n",
    "            self._convert_to_coco_format()\n",
    "            return images_dir, ann_path\n",
    "        \n",
    "        # Fallback: create synthetic dataset\n",
    "        print(\"Using synthetic dataset as fallback...\")\n",
    "        self.create_synthetic_thermal_dataset(num_images=250)\n",
    "        return images_dir, ann_path\n",
    "    \n",
    "    def _convert_to_coco_format(self):\n",
    "        \"\"\"Convert HIT-UAV annotations to COCO format (person class only).\"\"\"\n",
    "        # HIT-UAV uses YOLO format or custom XML\n",
    "        # This handles conversion to standard COCO JSON\n",
    "        \n",
    "        ann_path = self.data_root / \"annotations.json\"\n",
    "        if ann_path.exists():\n",
    "            return  # Already converted\n",
    "        \n",
    "        images_dir = self.data_root / \"images\"\n",
    "        labels_dir = self.data_root / \"labels\"  # YOLO format\n",
    "        \n",
    "        annotations = {\n",
    "            \"images\": [],\n",
    "            \"annotations\": [],\n",
    "            \"categories\": [{\"id\": 1, \"name\": \"person\", \"supercategory\": \"human\"}]\n",
    "        }\n",
    "        \n",
    "        ann_id = 1\n",
    "        \n",
    "        # HIT-UAV class mapping: 0=Person, 1=Car, 2=Bicycle, etc.\n",
    "        PERSON_CLASS = 0\n",
    "        \n",
    "        image_files = sorted(images_dir.glob(\"*.jpg\")) + sorted(images_dir.glob(\"*.png\"))\n",
    "        \n",
    "        for img_id, img_path in enumerate(image_files, 1):\n",
    "            # Read image dimensions\n",
    "            img = cv2.imread(str(img_path))\n",
    "            if img is None:\n",
    "                continue\n",
    "            h, w = img.shape[:2]\n",
    "            \n",
    "            annotations[\"images\"].append({\n",
    "                \"id\": img_id,\n",
    "                \"file_name\": img_path.name,\n",
    "                \"width\": w,\n",
    "                \"height\": h\n",
    "            })\n",
    "            \n",
    "            # Look for YOLO-format label file\n",
    "            label_path = labels_dir / f\"{img_path.stem}.txt\"\n",
    "            if label_path.exists():\n",
    "                with open(label_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) >= 5:\n",
    "                            cls_id = int(parts[0])\n",
    "                            if cls_id == PERSON_CLASS:\n",
    "                                # YOLO format: class cx cy width height (normalized)\n",
    "                                cx, cy, bw, bh = map(float, parts[1:5])\n",
    "                                \n",
    "                                # Convert to COCO format (x, y, width, height in pixels)\n",
    "                                x = (cx - bw/2) * w\n",
    "                                y = (cy - bh/2) * h\n",
    "                                box_w = bw * w\n",
    "                                box_h = bh * h\n",
    "                                \n",
    "                                annotations[\"annotations\"].append({\n",
    "                                    \"id\": ann_id,\n",
    "                                    \"image_id\": img_id,\n",
    "                                    \"category_id\": 1,\n",
    "                                    \"bbox\": [x, y, box_w, box_h],\n",
    "                                    \"area\": box_w * box_h,\n",
    "                                    \"iscrowd\": 0\n",
    "                                })\n",
    "                                ann_id += 1\n",
    "        \n",
    "        with open(ann_path, 'w') as f:\n",
    "            json.dump(annotations, f)\n",
    "        \n",
    "        print(f\"âœ“ Converted to COCO format: {len(annotations['images'])} images, {ann_id-1} person annotations\")\n",
    "\n",
    "\n",
    "# Run data ingestion\n",
    "downloader = DatasetDownloader(Config.DATA_ROOT)\n",
    "IMAGES_DIR, ANNOTATIONS_PATH = downloader.load_or_download()\n",
    "\n",
    "print(f\"\\nâœ“ Dataset ready at:\")\n",
    "print(f\"  Images: {IMAGES_DIR}\")\n",
    "print(f\"  Annotations: {ANNOTATIONS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Scene Filtering & Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: FILTERING & CURATION\n",
    "# Filter out urban/street scenes, create curated SAR-focused dataset\n",
    "# =============================================================================\n",
    "\n",
    "class SceneFilter:\n",
    "    \"\"\"Filter images to focus on SAR-relevant scenes (wilderness, disaster zones).\"\"\"\n",
    "    \n",
    "    # Keywords suggesting urban/street scenes to exclude\n",
    "    URBAN_KEYWORDS = ['street', 'road', 'traffic', 'parking', 'downtown', \n",
    "                      'intersection', 'highway', 'urban', 'city']\n",
    "    \n",
    "    # Keywords suggesting SAR-relevant scenes to keep\n",
    "    SAR_KEYWORDS = ['field', 'forest', 'mountain', 'wilderness', 'rural',\n",
    "                    'search', 'rescue', 'thermal', 'night']\n",
    "    \n",
    "    def __init__(self, ann_path: Path, images_dir: Path):\n",
    "        with open(ann_path, 'r') as f:\n",
    "            self.annotations = json.load(f)\n",
    "        self.images_dir = images_dir\n",
    "        \n",
    "    def filter_by_filename(self, filename: str) -> bool:\n",
    "        \"\"\"Heuristic filtering based on filename.\"\"\"\n",
    "        fname_lower = filename.lower()\n",
    "        \n",
    "        # Reject if contains urban keywords\n",
    "        for kw in self.URBAN_KEYWORDS:\n",
    "            if kw in fname_lower:\n",
    "                return False\n",
    "        \n",
    "        # Accept all synthetic images\n",
    "        if 'thermal_' in fname_lower or 'synthetic' in fname_lower:\n",
    "            return True\n",
    "            \n",
    "        return True  # Accept by default\n",
    "    \n",
    "    def filter_by_image_content(self, img: np.ndarray) -> bool:\n",
    "        \"\"\"\n",
    "        Simple content-based filtering using image statistics.\n",
    "        Urban scenes often have more structured patterns (buildings, roads).\n",
    "        \"\"\"\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img\n",
    "        \n",
    "        # Edge density: urban scenes tend to have more edges\n",
    "        edges = cv2.Canny(gray, 50, 150)\n",
    "        edge_density = np.sum(edges > 0) / edges.size\n",
    "        \n",
    "        # High edge density might indicate urban scene\n",
    "        if edge_density > 0.15:\n",
    "            return False\n",
    "        \n",
    "        # Check for horizontal/vertical line dominance (roads, buildings)\n",
    "        lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=50, \n",
    "                                minLineLength=50, maxLineGap=10)\n",
    "        if lines is not None and len(lines) > 20:\n",
    "            return False  # Too many straight lines\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def create_curated_dataset(\n",
    "        self, \n",
    "        output_dir: Path,\n",
    "        target_size: int = 512,\n",
    "        max_images: int = None,\n",
    "        use_content_filter: bool = False\n",
    "    ) -> Tuple[Path, Path]:\n",
    "        \"\"\"\n",
    "        Create filtered dataset with resized images.\n",
    "        \n",
    "        Args:\n",
    "            output_dir: Output directory\n",
    "            target_size: Resize images to this size\n",
    "            max_images: Limit number of images (for quick testing)\n",
    "            use_content_filter: Enable slow content-based filtering\n",
    "        \"\"\"\n",
    "        output_dir = Path(output_dir)\n",
    "        images_out = output_dir / \"images\"\n",
    "        images_out.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Build image id to annotations mapping\n",
    "        img_to_anns = defaultdict(list)\n",
    "        for ann in self.annotations['annotations']:\n",
    "            img_to_anns[ann['image_id']].append(ann)\n",
    "        \n",
    "        filtered_images = []\n",
    "        filtered_annotations = []\n",
    "        new_ann_id = 1\n",
    "        \n",
    "        images_list = self.annotations['images']\n",
    "        if max_images:\n",
    "            images_list = images_list[:max_images]\n",
    "        \n",
    "        print(f\"Filtering {len(images_list)} images...\")\n",
    "        \n",
    "        for img_info in tqdm(images_list, desc=\"Curating\"):\n",
    "            filename = img_info['file_name']\n",
    "            img_path = self.images_dir / filename\n",
    "            \n",
    "            # Skip if no file\n",
    "            if not img_path.exists():\n",
    "                continue\n",
    "            \n",
    "            # Skip if no person annotations\n",
    "            anns = img_to_anns.get(img_info['id'], [])\n",
    "            if len(anns) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Filename heuristic filter\n",
    "            if not self.filter_by_filename(filename):\n",
    "                continue\n",
    "            \n",
    "            # Load image\n",
    "            img = cv2.imread(str(img_path))\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            # Content-based filter (optional, slower)\n",
    "            if use_content_filter and not self.filter_by_image_content(img):\n",
    "                continue\n",
    "            \n",
    "            # Resize image\n",
    "            orig_h, orig_w = img.shape[:2]\n",
    "            scale_x = target_size / orig_w\n",
    "            scale_y = target_size / orig_h\n",
    "            \n",
    "            img_resized = cv2.resize(img, (target_size, target_size))\n",
    "            \n",
    "            # Save resized image\n",
    "            new_filename = f\"curated_{len(filtered_images):04d}.jpg\"\n",
    "            cv2.imwrite(str(images_out / new_filename), img_resized)\n",
    "            \n",
    "            new_img_id = len(filtered_images) + 1\n",
    "            filtered_images.append({\n",
    "                \"id\": new_img_id,\n",
    "                \"file_name\": new_filename,\n",
    "                \"width\": target_size,\n",
    "                \"height\": target_size,\n",
    "                \"original_file\": filename\n",
    "            })\n",
    "            \n",
    "            # Scale annotations\n",
    "            for ann in anns:\n",
    "                x, y, w, h = ann['bbox']\n",
    "                scaled_bbox = [\n",
    "                    x * scale_x,\n",
    "                    y * scale_y,\n",
    "                    w * scale_x,\n",
    "                    h * scale_y\n",
    "                ]\n",
    "                \n",
    "                # Skip tiny boxes after scaling\n",
    "                if scaled_bbox[2] < 5 or scaled_bbox[3] < 5:\n",
    "                    continue\n",
    "                \n",
    "                filtered_annotations.append({\n",
    "                    \"id\": new_ann_id,\n",
    "                    \"image_id\": new_img_id,\n",
    "                    \"category_id\": 1,\n",
    "                    \"bbox\": scaled_bbox,\n",
    "                    \"area\": scaled_bbox[2] * scaled_bbox[3],\n",
    "                    \"iscrowd\": 0\n",
    "                })\n",
    "                new_ann_id += 1\n",
    "        \n",
    "        # Save filtered annotations\n",
    "        filtered_coco = {\n",
    "            \"images\": filtered_images,\n",
    "            \"annotations\": filtered_annotations,\n",
    "            \"categories\": [{\"id\": 1, \"name\": \"person\", \"supercategory\": \"human\"}]\n",
    "        }\n",
    "        \n",
    "        ann_out = output_dir / \"instances_filtered.json\"\n",
    "        with open(ann_out, 'w') as f:\n",
    "            json.dump(filtered_coco, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nâœ“ Curated dataset created:\")\n",
    "        print(f\"  Images: {len(filtered_images)}\")\n",
    "        print(f\"  Person annotations: {len(filtered_annotations)}\")\n",
    "        print(f\"  Saved to: {output_dir}\")\n",
    "        \n",
    "        return images_out, ann_out\n",
    "\n",
    "\n",
    "# Create curated dataset\n",
    "scene_filter = SceneFilter(ANNOTATIONS_PATH, IMAGES_DIR)\n",
    "CURATED_IMAGES, CURATED_ANNOTATIONS = scene_filter.create_curated_dataset(\n",
    "    output_dir=Path(Config.CURATED_ROOT),\n",
    "    target_size=Config.IMG_SIZE,\n",
    "    use_content_filter=False  # Set True for stricter filtering (slower)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: SAR Augmentations (Snow, Smoke/Fire, Thermal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: SAR-SPECIFIC AUGMENTATIONS\n",
    "# Realistic augmentations for search-and-rescue conditions\n",
    "# =============================================================================\n",
    "\n",
    "class SARaugmentations:\n",
    "    \"\"\"Realistic augmentations for SAR drone imagery.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_perlin_noise(shape: Tuple[int, int], scale: float = 100.0) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate Perlin-like noise using multiple octaves of Gaussian noise.\n",
    "        Used for natural-looking snow and smoke patterns.\n",
    "        \"\"\"\n",
    "        h, w = shape\n",
    "        noise = np.zeros((h, w), dtype=np.float32)\n",
    "        \n",
    "        # Multiple octaves for natural appearance\n",
    "        for octave in range(4):\n",
    "            freq = 2 ** octave\n",
    "            amplitude = 1.0 / freq\n",
    "            \n",
    "            # Generate low-res noise and upscale\n",
    "            small_h = max(2, int(h / (scale / freq)))\n",
    "            small_w = max(2, int(w / (scale / freq)))\n",
    "            \n",
    "            small_noise = np.random.randn(small_h, small_w).astype(np.float32)\n",
    "            upscaled = cv2.resize(small_noise, (w, h), interpolation=cv2.INTER_CUBIC)\n",
    "            \n",
    "            noise += amplitude * upscaled\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        noise = (noise - noise.min()) / (noise.max() - noise.min() + 1e-8)\n",
    "        return noise\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_snow(\n",
    "        img: np.ndarray,\n",
    "        intensity: float = 0.5,\n",
    "        flake_size: float = 2.0,\n",
    "        contrast_reduction: float = 0.3\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply realistic snow effect.\n",
    "        \n",
    "        Args:\n",
    "            img: Input image (BGR or grayscale)\n",
    "            intensity: Overall snow intensity [0-1]\n",
    "            flake_size: Size of snow particles\n",
    "            contrast_reduction: How much to reduce contrast [0-1]\n",
    "        \"\"\"\n",
    "        h, w = img.shape[:2]\n",
    "        is_color = len(img.shape) == 3\n",
    "        \n",
    "        # Generate snow noise pattern\n",
    "        snow_noise = SARaugmentations.generate_perlin_noise((h, w), scale=50.0)\n",
    "        \n",
    "        # Add fine grain for individual flakes\n",
    "        fine_noise = np.random.rand(h, w).astype(np.float32)\n",
    "        fine_noise = cv2.GaussianBlur(fine_noise, (int(flake_size*2+1), int(flake_size*2+1)), 0)\n",
    "        \n",
    "        # Combine noises\n",
    "        snow_layer = 0.6 * snow_noise + 0.4 * fine_noise\n",
    "        snow_layer = np.clip(snow_layer * intensity * 255, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        if is_color:\n",
    "            snow_layer = cv2.cvtColor(snow_layer, cv2.COLOR_GRAY2BGR)\n",
    "        \n",
    "        # Reduce contrast (simulates atmospheric haze during snow)\n",
    "        img_float = img.astype(np.float32)\n",
    "        mean_val = np.mean(img_float)\n",
    "        img_reduced = (1 - contrast_reduction) * img_float + contrast_reduction * mean_val\n",
    "        \n",
    "        # Alpha blend snow\n",
    "        alpha = intensity * 0.7\n",
    "        result = (1 - alpha) * img_reduced + alpha * snow_layer.astype(np.float32)\n",
    "        \n",
    "        return np.clip(result, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_smoke_fire(\n",
    "        img: np.ndarray,\n",
    "        smoke_intensity: float = 0.4,\n",
    "        fire_intensity: float = 0.3,\n",
    "        fire_location: str = 'random'  # 'random', 'bottom', 'corner'\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply smoke and fire effects.\n",
    "        \n",
    "        Args:\n",
    "            img: Input image\n",
    "            smoke_intensity: Opacity of smoke [0-1]\n",
    "            fire_intensity: Intensity of fire glow [0-1]\n",
    "            fire_location: Where to place fire hotspot\n",
    "        \"\"\"\n",
    "        h, w = img.shape[:2]\n",
    "        is_color = len(img.shape) == 3\n",
    "        \n",
    "        if not is_color:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "        \n",
    "        result = img.astype(np.float32)\n",
    "        \n",
    "        # Generate fractal smoke pattern\n",
    "        smoke_noise = SARaugmentations.generate_perlin_noise((h, w), scale=80.0)\n",
    "        \n",
    "        # Smoke rises - gradient mask\n",
    "        gradient = np.linspace(1.0, 0.3, h).reshape(-1, 1)\n",
    "        gradient = np.tile(gradient, (1, w))\n",
    "        smoke_mask = smoke_noise * gradient\n",
    "        \n",
    "        # Apply smoke (gray overlay with blur)\n",
    "        smoke_color = np.array([180, 180, 180], dtype=np.float32)  # Gray smoke\n",
    "        smoke_layer = np.ones((h, w, 3), dtype=np.float32) * smoke_color\n",
    "        smoke_layer = cv2.GaussianBlur(smoke_layer, (21, 21), 0)\n",
    "        \n",
    "        smoke_alpha = smoke_mask[..., np.newaxis] * smoke_intensity\n",
    "        result = result * (1 - smoke_alpha) + smoke_layer * smoke_alpha\n",
    "        \n",
    "        # Add fire glow\n",
    "        if fire_intensity > 0:\n",
    "            # Fire hotspot location\n",
    "            if fire_location == 'random':\n",
    "                fx, fy = np.random.randint(w//4, 3*w//4), np.random.randint(h//2, h)\n",
    "            elif fire_location == 'bottom':\n",
    "                fx, fy = w//2, int(h * 0.8)\n",
    "            else:  # corner\n",
    "                fx, fy = int(w * 0.2), int(h * 0.8)\n",
    "            \n",
    "            # Create fire glow gradient\n",
    "            y_coords, x_coords = np.ogrid[:h, :w]\n",
    "            dist = np.sqrt((x_coords - fx)**2 + (y_coords - fy)**2)\n",
    "            fire_radius = min(h, w) // 3\n",
    "            fire_mask = np.clip(1 - dist / fire_radius, 0, 1) ** 2\n",
    "            \n",
    "            # Fire colors (orange-red in BGR)\n",
    "            fire_color = np.array([30, 100, 255], dtype=np.float32)  # Orange\n",
    "            fire_layer = np.ones((h, w, 3), dtype=np.float32) * fire_color\n",
    "            \n",
    "            fire_alpha = fire_mask[..., np.newaxis] * fire_intensity\n",
    "            result = result * (1 - fire_alpha) + fire_layer * fire_alpha\n",
    "        \n",
    "        return np.clip(result, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_thermal_artifacts(\n",
    "        img: np.ndarray,\n",
    "        intensity_scale: float = 1.0,\n",
    "        sensor_noise: float = 0.05,\n",
    "        local_saturation: float = 0.1\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply thermal camera artifacts.\n",
    "        \n",
    "        Args:\n",
    "            img: Input image\n",
    "            intensity_scale: Global intensity scaling factor\n",
    "            sensor_noise: Amount of sensor noise [0-1]\n",
    "            local_saturation: Probability of local saturation spots\n",
    "        \"\"\"\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        # Convert to grayscale for thermal processing\n",
    "        if len(img.shape) == 3:\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray = img.copy()\n",
    "        \n",
    "        result = gray.astype(np.float32)\n",
    "        \n",
    "        # Intensity scaling (simulates different thermal ranges)\n",
    "        result = result * intensity_scale\n",
    "        \n",
    "        # Add sensor noise (thermal cameras have characteristic noise)\n",
    "        noise = np.random.normal(0, sensor_noise * 255, (h, w)).astype(np.float32)\n",
    "        \n",
    "        # Temporal noise pattern (horizontal lines common in thermal)\n",
    "        if np.random.rand() < 0.3:\n",
    "            line_noise = np.random.normal(0, sensor_noise * 100, (h, 1))\n",
    "            line_noise = np.tile(line_noise, (1, w))\n",
    "            noise += line_noise\n",
    "        \n",
    "        result += noise\n",
    "        \n",
    "        # Local saturation spots (hot/cold spots)\n",
    "        if np.random.rand() < local_saturation:\n",
    "            num_spots = np.random.randint(1, 4)\n",
    "            for _ in range(num_spots):\n",
    "                sx, sy = np.random.randint(0, w), np.random.randint(0, h)\n",
    "                radius = np.random.randint(5, 30)\n",
    "                value = 255 if np.random.rand() > 0.5 else 0  # Hot or cold\n",
    "                cv2.circle(result, (sx, sy), radius, value, -1)\n",
    "        \n",
    "        result = np.clip(result, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        # Convert back to 3-channel\n",
    "        return cv2.cvtColor(result, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_random_sar_augmentation(\n",
    "        img: np.ndarray,\n",
    "        augmentation_type: str = 'random'\n",
    "    ) -> Tuple[np.ndarray, str]:\n",
    "        \"\"\"\n",
    "        Apply random SAR augmentation.\n",
    "        \n",
    "        Args:\n",
    "            img: Input image\n",
    "            augmentation_type: 'snow', 'fire', 'thermal', or 'random'\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (augmented image, augmentation name)\n",
    "        \"\"\"\n",
    "        if augmentation_type == 'random':\n",
    "            augmentation_type = np.random.choice(['snow', 'fire', 'thermal', 'clean'])\n",
    "        \n",
    "        if augmentation_type == 'snow':\n",
    "            intensity = np.random.uniform(0.3, 0.7)\n",
    "            return SARaugmentations.apply_snow(img, intensity=intensity), 'snow'\n",
    "        \n",
    "        elif augmentation_type == 'fire':\n",
    "            smoke = np.random.uniform(0.2, 0.5)\n",
    "            fire = np.random.uniform(0.2, 0.5)\n",
    "            return SARaugmentations.apply_smoke_fire(img, smoke, fire), 'fire'\n",
    "        \n",
    "        elif augmentation_type == 'thermal':\n",
    "            scale = np.random.uniform(0.8, 1.2)\n",
    "            noise = np.random.uniform(0.02, 0.1)\n",
    "            return SARaugmentations.apply_thermal_artifacts(img, scale, noise), 'thermal'\n",
    "        \n",
    "        else:\n",
    "            return img, 'clean'\n",
    "\n",
    "\n",
    "def create_perturbed_test_set(\n",
    "    images_dir: Path,\n",
    "    annotations_path: Path,\n",
    "    output_dir: Path,\n",
    "    perturbation_types: List[str] = ['snow', 'fire']\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Create a perturbed version of the test set for robustness evaluation.\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(annotations_path, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    \n",
    "    print(f\"Creating perturbed test set with: {perturbation_types}\")\n",
    "    \n",
    "    for img_info in tqdm(annotations['images'], desc=\"Perturbing\"):\n",
    "        img_path = images_dir / img_info['file_name']\n",
    "        img = cv2.imread(str(img_path))\n",
    "        \n",
    "        if img is None:\n",
    "            continue\n",
    "        \n",
    "        # Apply random perturbation\n",
    "        aug_type = np.random.choice(perturbation_types)\n",
    "        perturbed, _ = SARaugmentations.apply_random_sar_augmentation(img, aug_type)\n",
    "        \n",
    "        # Save\n",
    "        cv2.imwrite(str(output_dir / img_info['file_name']), perturbed)\n",
    "    \n",
    "    # Copy annotations (same boxes, different images)\n",
    "    shutil.copy(annotations_path, output_dir / \"instances_perturbed.json\")\n",
    "    \n",
    "    print(f\"âœ“ Perturbed test set saved to {output_dir}\")\n",
    "    return output_dir\n",
    "\n",
    "\n",
    "# Demonstrate augmentations\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAR AUGMENTATION EXAMPLES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load a sample image\n",
    "sample_images = list(CURATED_IMAGES.glob(\"*.jpg\"))[:1]\n",
    "if sample_images:\n",
    "    sample_img = cv2.imread(str(sample_images[0]))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "    \n",
    "    axes[0, 0].imshow(cv2.cvtColor(sample_img, cv2.COLOR_BGR2RGB))\n",
    "    axes[0, 0].set_title('Original')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    snow_img = SARaugmentations.apply_snow(sample_img, intensity=0.5)\n",
    "    axes[0, 1].imshow(cv2.cvtColor(snow_img, cv2.COLOR_BGR2RGB))\n",
    "    axes[0, 1].set_title('Snow Effect')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    fire_img = SARaugmentations.apply_smoke_fire(sample_img, smoke_intensity=0.4, fire_intensity=0.4)\n",
    "    axes[1, 0].imshow(cv2.cvtColor(fire_img, cv2.COLOR_BGR2RGB))\n",
    "    axes[1, 0].set_title('Smoke/Fire Effect')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    thermal_img = SARaugmentations.apply_thermal_artifacts(sample_img, intensity_scale=1.1, sensor_noise=0.08)\n",
    "    axes[1, 1].imshow(cv2.cvtColor(thermal_img, cv2.COLOR_BGR2RGB))\n",
    "    axes[1, 1].set_title('Thermal Artifacts')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{Config.OUTPUT_DIR}/augmentation_examples.png\", dpi=150)\n",
    "    plt.show()\n",
    "    print(f\"âœ“ Augmentation examples saved to {Config.OUTPUT_DIR}/augmentation_examples.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: PYTORCH DATASET AND DATALOADER\n",
    "# Custom dataset with on-the-fly augmentation support\n",
    "# =============================================================================\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "class UAVPersonDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for UAV person detection.\n",
    "    \n",
    "    Supports:\n",
    "    - COCO-format annotations\n",
    "    - On-the-fly SAR augmentations\n",
    "    - Standard albumentations transforms\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        images_dir: Path,\n",
    "        annotations_path: Path,\n",
    "        transform=None,\n",
    "        apply_sar_augmentation: bool = False,\n",
    "        sar_aug_prob: float = 0.5\n",
    "    ):\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.transform = transform\n",
    "        self.apply_sar_augmentation = apply_sar_augmentation\n",
    "        self.sar_aug_prob = sar_aug_prob\n",
    "        \n",
    "        # Load COCO annotations\n",
    "        with open(annotations_path, 'r') as f:\n",
    "            coco = json.load(f)\n",
    "        \n",
    "        self.images = {img['id']: img for img in coco['images']}\n",
    "        \n",
    "        # Group annotations by image\n",
    "        self.img_to_anns = defaultdict(list)\n",
    "        for ann in coco['annotations']:\n",
    "            self.img_to_anns[ann['image_id']].append(ann)\n",
    "        \n",
    "        # Only keep images with annotations\n",
    "        self.img_ids = [img_id for img_id in self.images.keys() \n",
    "                        if len(self.img_to_anns[img_id]) > 0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_info = self.images[img_id]\n",
    "        \n",
    "        # Load image\n",
    "        img_path = self.images_dir / img_info['file_name']\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Apply SAR augmentation\n",
    "        if self.apply_sar_augmentation and np.random.rand() < self.sar_aug_prob:\n",
    "            img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "            img_aug, _ = SARaugmentations.apply_random_sar_augmentation(img_bgr)\n",
    "            img = cv2.cvtColor(img_aug, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Get annotations\n",
    "        anns = self.img_to_anns[img_id]\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        areas = []\n",
    "        \n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            # Convert to [x1, y1, x2, y2] format\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(ann['category_id'])\n",
    "            areas.append(ann['area'])\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            transformed = self.transform(\n",
    "                image=img,\n",
    "                bboxes=boxes,\n",
    "                labels=labels\n",
    "            )\n",
    "            img = transformed['image']\n",
    "            boxes = transformed['bboxes']\n",
    "            labels = transformed['labels']\n",
    "        else:\n",
    "            # Default: just convert to tensor\n",
    "            img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
    "        \n",
    "        # Prepare target dict for Faster R-CNN\n",
    "        if len(boxes) > 0:\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        else:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "            areas = torch.zeros((0,), dtype=torch.float32)\n",
    "        \n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([img_id]),\n",
    "            'area': areas,\n",
    "            'iscrowd': torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "        }\n",
    "        \n",
    "        return img, target\n",
    "\n",
    "\n",
    "def get_transforms(train: bool = True):\n",
    "    \"\"\"Get albumentations transforms for training/validation.\"\"\"\n",
    "    if train:\n",
    "        return A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.3),\n",
    "            A.GaussNoise(var_limit=(10, 50), p=0.2),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            label_fields=['labels'],\n",
    "            min_visibility=0.3\n",
    "        ))\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            label_fields=['labels']\n",
    "        ))\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for detection.\"\"\"\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "# Create train/val split\n",
    "with open(CURATED_ANNOTATIONS, 'r') as f:\n",
    "    full_annotations = json.load(f)\n",
    "\n",
    "# 80/20 train/val split\n",
    "all_images = full_annotations['images']\n",
    "np.random.shuffle(all_images)\n",
    "split_idx = int(0.8 * len(all_images))\n",
    "\n",
    "train_images = all_images[:split_idx]\n",
    "val_images = all_images[split_idx:]\n",
    "\n",
    "train_img_ids = set(img['id'] for img in train_images)\n",
    "val_img_ids = set(img['id'] for img in val_images)\n",
    "\n",
    "train_anns = [a for a in full_annotations['annotations'] if a['image_id'] in train_img_ids]\n",
    "val_anns = [a for a in full_annotations['annotations'] if a['image_id'] in val_img_ids]\n",
    "\n",
    "# Save split annotations\n",
    "train_coco = {'images': train_images, 'annotations': train_anns, 'categories': full_annotations['categories']}\n",
    "val_coco = {'images': val_images, 'annotations': val_anns, 'categories': full_annotations['categories']}\n",
    "\n",
    "train_ann_path = Path(Config.CURATED_ROOT) / \"instances_train.json\"\n",
    "val_ann_path = Path(Config.CURATED_ROOT) / \"instances_val.json\"\n",
    "\n",
    "with open(train_ann_path, 'w') as f:\n",
    "    json.dump(train_coco, f)\n",
    "with open(val_ann_path, 'w') as f:\n",
    "    json.dump(val_coco, f)\n",
    "\n",
    "print(f\"\\nâœ“ Dataset split created:\")\n",
    "print(f\"  Train: {len(train_images)} images, {len(train_anns)} annotations\")\n",
    "print(f\"  Val: {len(val_images)} images, {len(val_anns)} annotations\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = UAVPersonDataset(\n",
    "    CURATED_IMAGES, train_ann_path,\n",
    "    transform=get_transforms(train=True),\n",
    "    apply_sar_augmentation=True,\n",
    "    sar_aug_prob=0.3\n",
    ")\n",
    "\n",
    "val_dataset = UAVPersonDataset(\n",
    "    CURATED_IMAGES, val_ann_path,\n",
    "    transform=get_transforms(train=False),\n",
    "    apply_sar_augmentation=False\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=Config.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=Config.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ DataLoaders created:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Model Setup (Faster R-CNN with Transfer Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 7: MODEL SETUP\n",
    "# Load pre-trained Faster R-CNN and adapt for person detection\n",
    "# =============================================================================\n",
    "\n",
    "# ============================================\n",
    "# ALTERNATIVE: To use YOLOv8 instead, uncomment:\n",
    "# from ultralytics import YOLO\n",
    "# model = YOLO('yolov8n.pt')  # or yolov8s.pt\n",
    "# # Fine-tune with: model.train(data='your_data.yaml', epochs=5)\n",
    "# ============================================\n",
    "\n",
    "def create_model(num_classes: int = 2, pretrained: bool = True, freeze_backbone: bool = True):\n",
    "    \"\"\"\n",
    "    Create Faster R-CNN model adapted for person detection.\n",
    "    \n",
    "    Args:\n",
    "        num_classes: Number of classes (including background)\n",
    "        pretrained: Use pretrained COCO weights\n",
    "        freeze_backbone: Freeze backbone layers initially\n",
    "    \"\"\"\n",
    "    # Load pretrained Faster R-CNN\n",
    "    if pretrained:\n",
    "        weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "        model = fasterrcnn_resnet50_fpn(weights=weights)\n",
    "        print(\"âœ“ Loaded pretrained Faster R-CNN (COCO weights)\")\n",
    "    else:\n",
    "        model = fasterrcnn_resnet50_fpn(weights=None)\n",
    "        print(\"âœ“ Created Faster R-CNN from scratch\")\n",
    "    \n",
    "    # Replace classification head for our classes\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    print(f\"âœ“ Replaced head for {num_classes} classes\")\n",
    "    \n",
    "    # Freeze backbone layers for initial training\n",
    "    if freeze_backbone:\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'backbone' in name:\n",
    "                # Freeze all except last layer of backbone\n",
    "                if 'layer4' not in name and 'fpn' not in name:\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        # Count trainable params\n",
    "        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        total = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"âœ“ Frozen early backbone layers\")\n",
    "        print(f\"  Trainable params: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = create_model(\n",
    "    num_classes=Config.NUM_CLASSES,\n",
    "    pretrained=True,\n",
    "    freeze_backbone=True\n",
    ")\n",
    "\n",
    "model.to(Config.DEVICE)\n",
    "print(f\"\\nâœ“ Model moved to {Config.DEVICE}\")\n",
    "\n",
    "# ============================================\n",
    "# FOR RGB DATASETS (e.g., VisDrone, Okutama):\n",
    "# The model works the same way. Key differences:\n",
    "# 1. No need for thermal-specific augmentations\n",
    "# 2. Use standard ImageNet normalization (already set)\n",
    "# 3. Consider adding color augmentations\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: TRAINING LOOP\n",
    "# Lightweight training with progress tracking\n",
    "# =============================================================================\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    pbar = tqdm(data_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    for images, targets in pbar:\n",
    "        # Move to device\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        # Skip empty batches\n",
    "        if len(images) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track loss\n",
    "        total_loss += losses.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{losses.item():.4f}\",\n",
    "            'avg': f\"{total_loss/num_batches:.4f}\"\n",
    "        })\n",
    "    \n",
    "    return total_loss / max(num_batches, 1)\n",
    "\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    \"\"\"Compute IoU between two boxes [x1, y1, x2, y2].\"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    \n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    \n",
    "    return inter_area / max(union_area, 1e-6)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device, iou_threshold=0.5, conf_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate model and compute precision, recall, F1 @ IoU threshold.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_tp = 0\n",
    "    all_fp = 0\n",
    "    all_fn = 0\n",
    "    \n",
    "    for images, targets in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "        images = [img.to(device) for img in images]\n",
    "        \n",
    "        # Get predictions\n",
    "        outputs = model(images)\n",
    "        \n",
    "        for output, target in zip(outputs, targets):\n",
    "            # Filter predictions by confidence\n",
    "            pred_boxes = output['boxes'][output['scores'] > conf_threshold].cpu().numpy()\n",
    "            gt_boxes = target['boxes'].cpu().numpy()\n",
    "            \n",
    "            # Match predictions to ground truth\n",
    "            matched_gt = set()\n",
    "            \n",
    "            for pred_box in pred_boxes:\n",
    "                best_iou = 0\n",
    "                best_gt_idx = -1\n",
    "                \n",
    "                for gt_idx, gt_box in enumerate(gt_boxes):\n",
    "                    if gt_idx in matched_gt:\n",
    "                        continue\n",
    "                    iou = compute_iou(pred_box, gt_box)\n",
    "                    if iou > best_iou:\n",
    "                        best_iou = iou\n",
    "                        best_gt_idx = gt_idx\n",
    "                \n",
    "                if best_iou >= iou_threshold:\n",
    "                    all_tp += 1\n",
    "                    matched_gt.add(best_gt_idx)\n",
    "                else:\n",
    "                    all_fp += 1\n",
    "            \n",
    "            # Count missed ground truth as false negatives\n",
    "            all_fn += len(gt_boxes) - len(matched_gt)\n",
    "    \n",
    "    # Compute metrics\n",
    "    precision = all_tp / max(all_tp + all_fp, 1)\n",
    "    recall = all_tp / max(all_tp + all_fn, 1)\n",
    "    f1 = 2 * precision * recall / max(precision + recall, 1e-6)\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'tp': all_tp,\n",
    "        'fp': all_fp,\n",
    "        'fn': all_fn\n",
    "    }\n",
    "\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.SGD(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    lr=Config.LR,\n",
    "    momentum=Config.MOMENTUM,\n",
    "    weight_decay=Config.WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_precision': [],\n",
    "    'val_recall': [],\n",
    "    'val_f1': []\n",
    "}\n",
    "\n",
    "best_f1 = 0\n",
    "\n",
    "for epoch in range(Config.NUM_EPOCHS):\n",
    "    # Train\n",
    "    train_loss = train_one_epoch(model, optimizer, train_loader, Config.DEVICE, epoch)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    \n",
    "    # Evaluate on clean validation set\n",
    "    metrics = evaluate(model, val_loader, Config.DEVICE, \n",
    "                       iou_threshold=Config.IOU_THRESHOLD,\n",
    "                       conf_threshold=Config.CONF_THRESHOLD)\n",
    "    \n",
    "    history['val_precision'].append(metrics['precision'])\n",
    "    history['val_recall'].append(metrics['recall'])\n",
    "    history['val_f1'].append(metrics['f1'])\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\nEpoch {epoch+1}/{Config.NUM_EPOCHS}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Val Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"  Val F1: {metrics['f1']:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if metrics['f1'] > best_f1:\n",
    "        best_f1 = metrics['f1']\n",
    "        checkpoint_path = f\"{Config.CHECKPOINT_DIR}/best_model.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'f1': best_f1,\n",
    "            'metrics': metrics\n",
    "        }, checkpoint_path)\n",
    "        print(f\"  âœ“ Saved best model (F1: {best_f1:.4f})\")\n",
    "\n",
    "print(f\"\\nâœ“ Training complete! Best F1: {best_f1:.4f}\")\n",
    "print(f\"  Checkpoint saved to: {Config.CHECKPOINT_DIR}/best_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Robustness Evaluation (Perturbed Test Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 9: ROBUSTNESS EVALUATION\n",
    "# Evaluate on synthetically perturbed data (snow/fire)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ROBUSTNESS EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create perturbed validation set\n",
    "perturbed_dir = Path(Config.CURATED_ROOT) / \"perturbed_val\"\n",
    "perturbed_images_dir = perturbed_dir / \"images\"\n",
    "perturbed_images_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load validation annotations\n",
    "with open(val_ann_path, 'r') as f:\n",
    "    val_annotations = json.load(f)\n",
    "\n",
    "print(f\"Creating perturbed validation images...\")\n",
    "\n",
    "for img_info in tqdm(val_annotations['images'], desc=\"Perturbing\"):\n",
    "    img_path = CURATED_IMAGES / img_info['file_name']\n",
    "    img = cv2.imread(str(img_path))\n",
    "    \n",
    "    if img is None:\n",
    "        continue\n",
    "    \n",
    "    # Apply random snow or fire perturbation\n",
    "    aug_type = np.random.choice(['snow', 'fire'])\n",
    "    perturbed, _ = SARaugmentations.apply_random_sar_augmentation(img, aug_type)\n",
    "    \n",
    "    # Save perturbed image\n",
    "    cv2.imwrite(str(perturbed_images_dir / img_info['file_name']), perturbed)\n",
    "\n",
    "# Save annotations for perturbed set\n",
    "perturbed_ann_path = perturbed_dir / \"instances_perturbed.json\"\n",
    "with open(perturbed_ann_path, 'w') as f:\n",
    "    json.dump(val_annotations, f)\n",
    "\n",
    "print(f\"âœ“ Perturbed validation set created: {perturbed_dir}\")\n",
    "\n",
    "# Create perturbed dataset and loader\n",
    "perturbed_dataset = UAVPersonDataset(\n",
    "    perturbed_images_dir, \n",
    "    perturbed_ann_path,\n",
    "    transform=get_transforms(train=False),\n",
    "    apply_sar_augmentation=False\n",
    ")\n",
    "\n",
    "perturbed_loader = DataLoader(\n",
    "    perturbed_dataset,\n",
    "    batch_size=Config.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load(f\"{Config.CHECKPOINT_DIR}/best_model.pth\", map_location=Config.DEVICE)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"\\nâœ“ Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "\n",
    "# Evaluate on clean validation set\n",
    "print(\"\\nEvaluating on CLEAN validation set...\")\n",
    "clean_metrics = evaluate(model, val_loader, Config.DEVICE,\n",
    "                         iou_threshold=Config.IOU_THRESHOLD,\n",
    "                         conf_threshold=Config.CONF_THRESHOLD)\n",
    "\n",
    "# Evaluate on perturbed validation set\n",
    "print(\"\\nEvaluating on PERTURBED validation set...\")\n",
    "perturbed_metrics = evaluate(model, perturbed_loader, Config.DEVICE,\n",
    "                             iou_threshold=Config.IOU_THRESHOLD,\n",
    "                             conf_threshold=Config.CONF_THRESHOLD)\n",
    "\n",
    "# Print comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ROBUSTNESS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<15} {'Clean':<12} {'Perturbed':<12} {'Delta':<12}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Precision':<15} {clean_metrics['precision']:.4f}       {perturbed_metrics['precision']:.4f}       {perturbed_metrics['precision']-clean_metrics['precision']:+.4f}\")\n",
    "print(f\"{'Recall':<15} {clean_metrics['recall']:.4f}       {perturbed_metrics['recall']:.4f}       {perturbed_metrics['recall']-clean_metrics['recall']:+.4f}\")\n",
    "print(f\"{'F1 Score':<15} {clean_metrics['f1']:.4f}       {perturbed_metrics['f1']:.4f}       {perturbed_metrics['f1']-clean_metrics['f1']:+.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "robustness_drop = (clean_metrics['f1'] - perturbed_metrics['f1']) / max(clean_metrics['f1'], 1e-6) * 100\n",
    "print(f\"\\nRobustness drop: {robustness_drop:.1f}%\")\n",
    "\n",
    "if robustness_drop < 10:\n",
    "    print(\"âœ“ Model shows good robustness to SAR conditions!\")\n",
    "elif robustness_drop < 25:\n",
    "    print(\"âš  Model shows moderate sensitivity to SAR conditions\")\n",
    "else:\n",
    "    print(\"âš  Model significantly degrades under SAR conditions - consider more augmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 10: VISUALIZATIONS\n",
    "# Show predictions on clean vs perturbed images\n",
    "# =============================================================================\n",
    "\n",
    "def visualize_predictions(\n",
    "    model,\n",
    "    images_dir: Path,\n",
    "    annotations_path: Path,\n",
    "    num_samples: int = 4,\n",
    "    conf_threshold: float = 0.5,\n",
    "    title_prefix: str = \"\"\n",
    "):\n",
    "    \"\"\"Visualize model predictions on sample images.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with open(annotations_path, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    \n",
    "    # Group annotations by image\n",
    "    img_to_anns = defaultdict(list)\n",
    "    for ann in annotations['annotations']:\n",
    "        img_to_anns[ann['image_id']].append(ann)\n",
    "    \n",
    "    # Sample images\n",
    "    sample_images = random.sample(annotations['images'], min(num_samples, len(annotations['images'])))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(4*num_samples, 8))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    for idx, img_info in enumerate(sample_images):\n",
    "        img_path = images_dir / img_info['file_name']\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Prepare for model\n",
    "        img_tensor = torch.from_numpy(img_rgb).permute(2, 0, 1).float() / 255.0\n",
    "        img_tensor = img_tensor.to(Config.DEVICE)\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            output = model([img_tensor])[0]\n",
    "        \n",
    "        # Ground truth\n",
    "        gt_img = img_rgb.copy()\n",
    "        gt_anns = img_to_anns.get(img_info['id'], [])\n",
    "        for ann in gt_anns:\n",
    "            x, y, w, h = map(int, ann['bbox'])\n",
    "            cv2.rectangle(gt_img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        \n",
    "        axes[0, idx].imshow(gt_img)\n",
    "        axes[0, idx].set_title(f\"{title_prefix}GT ({len(gt_anns)} persons)\")\n",
    "        axes[0, idx].axis('off')\n",
    "        \n",
    "        # Predictions\n",
    "        pred_img = img_rgb.copy()\n",
    "        pred_boxes = output['boxes'][output['scores'] > conf_threshold].cpu().numpy()\n",
    "        pred_scores = output['scores'][output['scores'] > conf_threshold].cpu().numpy()\n",
    "        \n",
    "        for box, score in zip(pred_boxes, pred_scores):\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            cv2.rectangle(pred_img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "            cv2.putText(pred_img, f\"{score:.2f}\", (x1, y1-5), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "        \n",
    "        axes[1, idx].imshow(pred_img)\n",
    "        axes[1, idx].set_title(f\"{title_prefix}Pred ({len(pred_boxes)} detections)\")\n",
    "        axes[1, idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VISUALIZATION: CLEAN VS PERTURBED\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Visualize clean images\n",
    "print(\"\\nClean validation set predictions:\")\n",
    "fig_clean = visualize_predictions(\n",
    "    model, CURATED_IMAGES, val_ann_path,\n",
    "    num_samples=4, conf_threshold=Config.CONF_THRESHOLD,\n",
    "    title_prefix=\"Clean: \"\n",
    ")\n",
    "fig_clean.savefig(f\"{Config.OUTPUT_DIR}/predictions_clean.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Visualize perturbed images\n",
    "print(\"\\nPerturbed validation set predictions:\")\n",
    "fig_perturbed = visualize_predictions(\n",
    "    model, perturbed_images_dir, perturbed_ann_path,\n",
    "    num_samples=4, conf_threshold=Config.CONF_THRESHOLD,\n",
    "    title_prefix=\"Perturbed: \"\n",
    ")\n",
    "fig_perturbed.savefig(f\"{Config.OUTPUT_DIR}/predictions_perturbed.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Training curves\n",
    "print(\"\\nTraining history:\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(history['train_loss'], 'b-o', label='Train Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(history['val_precision'], 'g-o', label='Precision')\n",
    "axes[1].plot(history['val_recall'], 'b-s', label='Recall')\n",
    "axes[1].plot(history['val_f1'], 'r-^', label='F1')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Validation Metrics')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{Config.OUTPUT_DIR}/training_history.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Visualizations saved to {Config.OUTPUT_DIR}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Save Outputs & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 11: SAVE OUTPUTS & SUMMARY\n",
    "# Save all artifacts to Drive for persistence\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING OUTPUTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save sample perturbed images\n",
    "sample_perturbed_dir = Path(Config.OUTPUT_DIR) / \"sample_perturbed\"\n",
    "sample_perturbed_dir.mkdir(exist_ok=True)\n",
    "\n",
    "perturbed_files = list(perturbed_images_dir.glob(\"*.jpg\"))[:10]\n",
    "for f in perturbed_files:\n",
    "    shutil.copy(f, sample_perturbed_dir / f.name)\n",
    "\n",
    "print(f\"âœ“ Sample perturbed images saved to {sample_perturbed_dir}\")\n",
    "\n",
    "# Copy filtered annotations\n",
    "shutil.copy(CURATED_ANNOTATIONS, f\"{Config.OUTPUT_DIR}/instances_filtered.json\")\n",
    "print(f\"âœ“ Filtered annotations saved to {Config.OUTPUT_DIR}/instances_filtered.json\")\n",
    "\n",
    "# Save metrics summary\n",
    "summary = {\n",
    "    'config': {\n",
    "        'image_size': Config.IMG_SIZE,\n",
    "        'num_epochs': Config.NUM_EPOCHS,\n",
    "        'batch_size': Config.BATCH_SIZE,\n",
    "        'learning_rate': Config.LR\n",
    "    },\n",
    "    'dataset': {\n",
    "        'train_images': len(train_images),\n",
    "        'val_images': len(val_images),\n",
    "        'train_annotations': len(train_anns),\n",
    "        'val_annotations': len(val_anns)\n",
    "    },\n",
    "    'training_history': history,\n",
    "    'best_epoch': checkpoint['epoch'] + 1,\n",
    "    'clean_metrics': clean_metrics,\n",
    "    'perturbed_metrics': perturbed_metrics,\n",
    "    'robustness_drop_percent': robustness_drop\n",
    "}\n",
    "\n",
    "with open(f\"{Config.OUTPUT_DIR}/experiment_summary.json\", 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Experiment summary saved to {Config.OUTPUT_DIR}/experiment_summary.json\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\"\"\n",
    "Dataset:\n",
    "  - Training images: {len(train_images)}\n",
    "  - Validation images: {len(val_images)}\n",
    "  - Image size: {Config.IMG_SIZE}x{Config.IMG_SIZE}\n",
    "\n",
    "Training:\n",
    "  - Epochs: {Config.NUM_EPOCHS}\n",
    "  - Best epoch: {checkpoint['epoch'] + 1}\n",
    "  - Final train loss: {history['train_loss'][-1]:.4f}\n",
    "\n",
    "Clean Validation Performance:\n",
    "  - Precision: {clean_metrics['precision']:.4f}\n",
    "  - Recall: {clean_metrics['recall']:.4f}\n",
    "  - F1: {clean_metrics['f1']:.4f}\n",
    "\n",
    "Perturbed Validation Performance:\n",
    "  - Precision: {perturbed_metrics['precision']:.4f}\n",
    "  - Recall: {perturbed_metrics['recall']:.4f}\n",
    "  - F1: {perturbed_metrics['f1']:.4f}\n",
    "\n",
    "Robustness:\n",
    "  - Performance drop: {robustness_drop:.1f}%\n",
    "\n",
    "Saved Artifacts:\n",
    "  - Checkpoint: {Config.CHECKPOINT_DIR}/best_model.pth\n",
    "  - Filtered annotations: {Config.OUTPUT_DIR}/instances_filtered.json\n",
    "  - Visualizations: {Config.OUTPUT_DIR}/*.png\n",
    "  - Summary: {Config.OUTPUT_DIR}/experiment_summary.json\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ DEMO READY! All outputs saved to Google Drive.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12: Quick Inference Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 12: QUICK INFERENCE DEMO\n",
    "# Run inference on a single image (for hackathon demo)\n",
    "# =============================================================================\n",
    "\n",
    "def run_inference(image_path: str, conf_threshold: float = 0.5):\n",
    "    \"\"\"\n",
    "    Run inference on a single image and display results.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to input image\n",
    "        conf_threshold: Confidence threshold for detections\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not load image {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # Resize to model input size\n",
    "    img_resized = cv2.resize(img, (Config.IMG_SIZE, Config.IMG_SIZE))\n",
    "    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Prepare tensor\n",
    "    img_tensor = torch.from_numpy(img_rgb).permute(2, 0, 1).float() / 255.0\n",
    "    img_tensor = img_tensor.to(Config.DEVICE)\n",
    "    \n",
    "    # Run inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model([img_tensor])[0]\n",
    "    \n",
    "    # Filter by confidence\n",
    "    mask = output['scores'] > conf_threshold\n",
    "    boxes = output['boxes'][mask].cpu().numpy()\n",
    "    scores = output['scores'][mask].cpu().numpy()\n",
    "    \n",
    "    # Draw detections\n",
    "    result_img = img_rgb.copy()\n",
    "    for box, score in zip(boxes, scores):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        cv2.rectangle(result_img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "        cv2.putText(result_img, f\"Person {score:.2f}\", (x1, y1-10),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "    \n",
    "    # Display\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(result_img)\n",
    "    plt.title(f\"Detected {len(boxes)} person(s)\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Found {len(boxes)} person(s) with confidence > {conf_threshold}\")\n",
    "    return boxes, scores\n",
    "\n",
    "\n",
    "# Demo: run on a sample image\n",
    "sample_files = list(CURATED_IMAGES.glob(\"*.jpg\"))\n",
    "if sample_files:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"QUICK INFERENCE DEMO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Run on random sample\n",
    "    sample_path = str(random.choice(sample_files))\n",
    "    print(f\"\\nRunning inference on: {sample_path}\")\n",
    "    run_inference(sample_path, conf_threshold=0.5)\n",
    "    \n",
    "    print(\"\\nðŸ’¡ To run on your own image:\")\n",
    "    print(\"   run_inference('/path/to/your/image.jpg')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“‹ Usage Notes\n",
    "\n",
    "### Swapping to RGB Datasets\n",
    "To use RGB datasets (e.g., VisDrone, Okutama):\n",
    "1. Replace the data download URL in `DatasetDownloader`\n",
    "2. Adjust the annotation conversion if not in COCO format\n",
    "3. Disable thermal-specific augmentations (`apply_sar_augmentation=False`)\n",
    "4. Consider adding color jitter augmentations for RGB\n",
    "\n",
    "### Swapping to YOLOv8\n",
    "Replace the model setup cell with:\n",
    "```python\n",
    "from ultralytics import YOLO\n",
    "model = YOLO('yolov8n.pt')  # or yolov8s.pt for better accuracy\n",
    "model.train(\n",
    "    data='your_data.yaml',\n",
    "    epochs=5,\n",
    "    imgsz=512,\n",
    "    batch=4\n",
    ")\n",
    "```\n",
    "\n",
    "### Quick 3-Minute Demo\n",
    "1. Run Cells 1-3 (setup + data)\n",
    "2. Skip to Cell 12 for inference on pre-trained model\n",
    "3. Show augmentation examples from Cell 5\n",
    "\n",
    "---\n",
    "\n",
    "**Created for SAR drone hackathon prototyping**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
