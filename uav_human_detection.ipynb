{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust Human Detection in UAV Imagery\n",
    "## HIT-UAV Infrared Thermal Dataset - Baseline vs Augmented Comparison\n",
    "\n",
    "**Experiment Design:**\n",
    "- **Model A**: Trained on clean/normal data only\n",
    "- **Model B**: Trained with SAR augmentations (snow, smoke/fire, thermal artifacts)\n",
    "- **Evaluation**: Compare both on clean and perturbed test sets\n",
    "\n",
    "**Dataset**: HIT-UAV from Kaggle (thermal infrared UAV imagery)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: ENVIRONMENT SETUP\n",
    "# =============================================================================\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Install required packages.\"\"\"\n",
    "    packages = [\n",
    "        'torch', 'torchvision', 'albumentations>=1.3.0', 'pycocotools',\n",
    "        'opencv-python-headless', 'matplotlib', 'numpy', 'Pillow',\n",
    "        'tqdm', 'scipy', 'kaggle'\n",
    "    ]\n",
    "    for pkg in packages:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pkg])\n",
    "    print(\"Packages installed\")\n",
    "\n",
    "install_packages()\n",
    "\n",
    "# Mount Google Drive (Colab) or use local cache\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DRIVE_ROOT = '/content/drive/MyDrive/uav_detection'\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    DRIVE_ROOT = './uav_detection_cache'\n",
    "    IN_COLAB = False\n",
    "\n",
    "os.makedirs(DRIVE_ROOT, exist_ok=True)\n",
    "os.makedirs(f\"{DRIVE_ROOT}/data\", exist_ok=True)\n",
    "os.makedirs(f\"{DRIVE_ROOT}/checkpoints\", exist_ok=True)\n",
    "os.makedirs(f\"{DRIVE_ROOT}/outputs\", exist_ok=True)\n",
    "\n",
    "print(f\"Cache directory: {DRIVE_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: IMPORTS AND CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    # Paths\n",
    "    DATA_ROOT = f\"{DRIVE_ROOT}/data/hit_uav\"\n",
    "    CURATED_ROOT = f\"{DRIVE_ROOT}/data/curated\"\n",
    "    CHECKPOINT_DIR = f\"{DRIVE_ROOT}/checkpoints\"\n",
    "    OUTPUT_DIR = f\"{DRIVE_ROOT}/outputs\"\n",
    "    \n",
    "    # Image settings\n",
    "    IMG_SIZE = 512\n",
    "    \n",
    "    # Training settings\n",
    "    BATCH_SIZE = 4\n",
    "    NUM_EPOCHS = 6\n",
    "    LR = 0.005\n",
    "    LR_STEP_SIZE = 3\n",
    "    LR_GAMMA = 0.1\n",
    "    WEIGHT_DECAY = 0.0005\n",
    "    \n",
    "    # Detection settings\n",
    "    NUM_CLASSES = 2  # background + person\n",
    "    IOU_THRESHOLD = 0.5\n",
    "    CONF_THRESHOLD = 0.5\n",
    "    \n",
    "    # Flags\n",
    "    USE_YOLO_DIRECT = True  # prefer YOLO txt labels and skip COCO conversion when possible\n",
    "    ENABLE_SYNTH_AUG = False  # disable synthetic SAR-style augmentations by default\n",
    "\n",
    "    # Device\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    SEED = 42\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(Config.SEED)\n",
    "np.random.seed(Config.SEED)\n",
    "random.seed(Config.SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(Config.SEED)\n",
    "\n",
    "print(f\"Device: {Config.DEVICE}\")\n",
    "print(f\"Image size: {Config.IMG_SIZE}, Epochs: {Config.NUM_EPOCHS}, Batch: {Config.BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Download HIT-UAV Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: DOWNLOAD HIT-UAV DATASET FROM KAGGLE\n",
    "# =============================================================================\n",
    "\n",
    "def download_hituav_kaggle(data_root: str) -> bool:\n",
    "    \"\"\"\n",
    "    Download HIT-UAV dataset from Kaggle.\n",
    "    Requires Kaggle API credentials (~/.kaggle/kaggle.json or KAGGLE_USERNAME/KAGGLE_KEY env vars)\n",
    "    \"\"\"\n",
    "    data_root = Path(data_root)\n",
    "    data_root.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Check if already downloaded\n",
    "    images_dir = data_root / \"normal\" / \"images\"\n",
    "    if images_dir.exists() and len(list(images_dir.glob(\"*.jpg\"))) > 100:\n",
    "        print(f\"Dataset already exists at {data_root}\")\n",
    "        return True\n",
    "    \n",
    "    zip_path = data_root / \"hituav.zip\"\n",
    "    \n",
    "    # Method 1: Try curl download\n",
    "    print(\"Downloading HIT-UAV dataset from Kaggle...\")\n",
    "    try:\n",
    "        import subprocess\n",
    "        result = subprocess.run([\n",
    "            'curl', '-L', '-o', str(zip_path),\n",
    "            'https://www.kaggle.com/api/v1/datasets/download/pandrii000/hituav-a-highaltitude-infrared-thermal-dataset'\n",
    "        ], capture_output=True, timeout=600)\n",
    "        \n",
    "        if zip_path.exists() and zip_path.stat().st_size > 1000000:  # >1MB\n",
    "            print(f\"Downloaded to {zip_path}\")\n",
    "        else:\n",
    "            raise Exception(\"Download failed or file too small\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Curl download failed: {e}\")\n",
    "        \n",
    "        # Method 2: Try kaggle API\n",
    "        try:\n",
    "            import kaggle\n",
    "            kaggle.api.dataset_download_files(\n",
    "                'pandrii000/hituav-a-highaltitude-infrared-thermal-dataset',\n",
    "                path=str(data_root),\n",
    "                unzip=False\n",
    "            )\n",
    "            # Find the downloaded zip\n",
    "            for f in data_root.glob(\"*.zip\"):\n",
    "                zip_path = f\n",
    "                break\n",
    "        except Exception as e2:\n",
    "            print(f\"Kaggle API failed: {e2}\")\n",
    "            print(\"\\nPlease download manually:\")\n",
    "            print(\"1. Go to: https://www.kaggle.com/datasets/pandrii000/hituav-a-highaltitude-infrared-thermal-dataset\")\n",
    "            print(\"2. Download and extract to:\", data_root)\n",
    "            return False\n",
    "    \n",
    "    # Extract\n",
    "    if zip_path.exists():\n",
    "        print(\"Extracting dataset...\")\n",
    "        try:\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "                zf.extractall(data_root)\n",
    "            print(f\"Extracted to {data_root}\")\n",
    "            zip_path.unlink()  # Remove zip\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Extraction failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def explore_dataset_structure(data_root: str):\n",
    "    \"\"\"Print the dataset structure to understand it.\"\"\"\n",
    "    data_root = Path(data_root)\n",
    "    print(f\"\\nDataset structure at {data_root}:\")\n",
    "    \n",
    "    for item in sorted(data_root.rglob(\"*\")):\n",
    "        if item.is_dir():\n",
    "            files = list(item.glob(\"*\"))\n",
    "            depth = len(item.relative_to(data_root).parts)\n",
    "            indent = \"  \" * depth\n",
    "            print(f\"{indent}{item.name}/ ({len(files)} items)\")\n",
    "            # Show sample files\n",
    "            if len(files) > 0 and files[0].is_file():\n",
    "                print(f\"{indent}  Sample: {files[0].name}\")\n",
    "\n",
    "\n",
    "def debug_dataset_content(data_root: str):\n",
    "    \"\"\"Debug: Show actual dataset content with images and labels.\"\"\"\n",
    "    data_root = Path(data_root)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DEBUG: Dataset Content Analysis\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Find images and labels directories\n",
    "    possible_paths = [\n",
    "        (data_root / \"normal\" / \"images\", data_root / \"normal\" / \"labels\"),\n",
    "        (data_root / \"train\" / \"images\", data_root / \"train\" / \"labels\"),\n",
    "        (data_root / \"images\", data_root / \"labels\"),\n",
    "    ]\n",
    "    \n",
    "    images_dir = None\n",
    "    labels_dir = None\n",
    "    \n",
    "    for img_dir, lbl_dir in possible_paths:\n",
    "        if img_dir.exists():\n",
    "            images_dir = img_dir\n",
    "            labels_dir = lbl_dir\n",
    "            print(f\"\\nFound images at: {images_dir}\")\n",
    "            print(f\"Found labels at: {labels_dir}\")\n",
    "            break\n",
    "    \n",
    "    if images_dir is None:\n",
    "        print(\"\\nERROR: Could not find images directory!\")\n",
    "        print(\"Available directories:\")\n",
    "        for item in data_root.iterdir():\n",
    "            if item.is_dir():\n",
    "                print(f\"  - {item}\")\n",
    "        return\n",
    "    \n",
    "    # Count files\n",
    "    image_files = list(images_dir.glob(\"*.jpg\")) + list(images_dir.glob(\"*.png\"))\n",
    "    print(f\"\\nTotal images found: {len(image_files)}\")\n",
    "    \n",
    "    if labels_dir.exists():\n",
    "        label_files = list(labels_dir.glob(\"*.txt\"))\n",
    "        print(f\"Total label files found: {len(label_files)}\")\n",
    "    else:\n",
    "        print(f\"WARNING: Labels directory does not exist: {labels_dir}\")\n",
    "        label_files = []\n",
    "    \n",
    "    # Analyze labels\n",
    "    if len(label_files) > 0:\n",
    "        print(\"\\nAnalyzing label content...\")\n",
    "        class_counts = defaultdict(int)\n",
    "        files_with_persons = 0\n",
    "        \n",
    "        for lbl_file in label_files[:100]:  # Sample first 100\n",
    "            with open(lbl_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 5:\n",
    "                        cls_id = int(parts[0])\n",
    "                        class_counts[cls_id] += 1\n",
    "                        if cls_id == 0:  # Person class\n",
    "                            files_with_persons += 1\n",
    "                            break\n",
    "        \n",
    "        print(f\"\\nClass distribution in first 100 label files:\")\n",
    "        for cls_id, count in sorted(class_counts.items()):\n",
    "            cls_name = {0: 'Person', 1: 'Car', 2: 'Bicycle', 3: 'OtherVehicle', 4: 'DontCare'}.get(cls_id, f'Unknown-{cls_id}')\n",
    "            print(f\"  Class {cls_id} ({cls_name}): {count} annotations\")\n",
    "        print(f\"  Files with persons (in first 100): {files_with_persons}\")\n",
    "        \n",
    "        # Show sample label file\n",
    "        if len(label_files) > 0:\n",
    "            sample_label = label_files[0]\n",
    "            print(f\"\\nSample label file: {sample_label.name}\")\n",
    "            with open(sample_label, 'r') as f:\n",
    "                lines = f.readlines()[:5]  # First 5 lines\n",
    "                for line in lines:\n",
    "                    print(f\"  {line.strip()}\")\n",
    "    \n",
    "    # Display sample images\n",
    "    if len(image_files) > 0:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"Displaying sample images from dataset...\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        sample_imgs = random.sample(image_files, min(4, len(image_files)))\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx, img_path in enumerate(sample_imgs):\n",
    "            img = cv2.imread(str(img_path))\n",
    "            if img is not None:\n",
    "                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Try to load corresponding labels\n",
    "                label_path = labels_dir / f\"{img_path.stem}.txt\"\n",
    "                person_count = 0\n",
    "                total_boxes = 0\n",
    "                \n",
    "                if label_path.exists():\n",
    "                    with open(label_path, 'r') as f:\n",
    "                        for line in f:\n",
    "                            parts = line.strip().split()\n",
    "                            if len(parts) >= 5:\n",
    "                                cls_id = int(parts[0])\n",
    "                                total_boxes += 1\n",
    "                                if cls_id == 0:\n",
    "                                    person_count += 1\n",
    "                                    # Draw bounding box\n",
    "                                    cx, cy, bw, bh = map(float, parts[1:5])\n",
    "                                    h, w = img_rgb.shape[:2]\n",
    "                                    x1 = int((cx - bw/2) * w)\n",
    "                                    y1 = int((cy - bh/2) * h)\n",
    "                                    x2 = int((cx + bw/2) * w)\n",
    "                                    y2 = int((cy + bh/2) * h)\n",
    "                                    cv2.rectangle(img_rgb, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                \n",
    "                axes[idx].imshow(img_rgb)\n",
    "                axes[idx].set_title(f'{img_path.name}\\nPersons: {person_count}, Total boxes: {total_boxes}')\n",
    "                axes[idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{Config.OUTPUT_DIR}/debug_dataset_samples.png\", dpi=150)\n",
    "        plt.show()\n",
    "        print(f\"Sample images saved to: {Config.OUTPUT_DIR}/debug_dataset_samples.png\")\n",
    "\n",
    "\n",
    "# Download dataset\n",
    "download_success = download_hituav_kaggle(Config.DATA_ROOT)\n",
    "\n",
    "if download_success:\n",
    "    explore_dataset_structure(Config.DATA_ROOT)\n",
    "    debug_dataset_content(Config.DATA_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Convert HIT-UAV to COCO Format (Person Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: CONVERT HIT-UAV TO COCO FORMAT (IF NEEDED)\n",
    "# HIT-UAV uses YOLO format: class_id cx cy w h (normalized)\n",
    "# We convert to COCO: {images: [], annotations: [], categories: []}\n",
    "\n",
    "# Helper: find YOLO-format images/labels already present\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "def find_hituav_yolo(data_root: str) -> Tuple[Optional[Path], Optional[Path]]:\n",
    "    candidates = [\n",
    "        (Path(data_root) / \"normal\" / \"images\", Path(data_root) / \"normal\" / \"labels\"),\n",
    "        (Path(data_root) / \"train\" / \"images\", Path(data_root) / \"train\" / \"labels\"),\n",
    "        (Path(data_root) / \"images\", Path(data_root) / \"labels\"),\n",
    "    ]\n",
    "    for img_dir, lbl_dir in candidates:\n",
    "        if img_dir.exists() and lbl_dir.exists():\n",
    "            image_files = list(img_dir.glob(\"*.jpg\")) + list(img_dir.glob(\"*.png\"))\n",
    "            label_files = list(lbl_dir.glob(\"*.txt\"))\n",
    "            if len(image_files) > 0 and len(label_files) > 0:\n",
    "                return img_dir, lbl_dir\n",
    "    return None, None\n",
    "# Only keeps 'Person' class (class_id=0 in HIT-UAV)\n",
    "# =============================================================================\n",
    "\n",
    "def convert_hituav_to_coco(data_root: str, output_path: str, target_size: int = 512, force_reconvert: bool = False) -> Tuple[Path, Path]:\n",
    "    \"\"\"\n",
    "    Convert HIT-UAV YOLO format to COCO JSON format.\n",
    "    Only keeps 'Person' class (class_id=0 in HIT-UAV).\n",
    "    \n",
    "    HIT-UAV classes: 0=Person, 1=Car, 2=Bicycle, 3=OtherVehicle, 4=DontCare\n",
    "    \"\"\"\n",
    "    data_root = Path(data_root)\n",
    "    output_path = Path(output_path)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    images_out = output_path / \"images\"\n",
    "    ann_path = output_path / \"annotations.json\"\n",
    "    \n",
    "    # Check if already converted\n",
    "    if not force_reconvert and ann_path.exists():\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"COCO annotations already exist at: {ann_path}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        with open(ann_path, 'r') as f:\n",
    "            coco = json.load(f)\n",
    "        \n",
    "        num_images = len(coco['images'])\n",
    "        num_anns = len(coco['annotations'])\n",
    "        \n",
    "        if num_images > 0:\n",
    "            print(f\"Existing COCO dataset:\")\n",
    "            print(f\"  - Images: {num_images}\")\n",
    "            print(f\"  - Annotations: {num_anns}\")\n",
    "            print(f\"  - Avg annotations per image: {num_anns/num_images:.1f}\")\n",
    "            print(f\"\\nSkipping conversion. Set force_reconvert=True to reconvert.\")\n",
    "            return images_out, ann_path\n",
    "        else:\n",
    "            print(f\"WARNING: Existing annotations file is empty. Reconverting...\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Converting HIT-UAV to COCO format (Person class only)...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    images_out.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Find the images and labels directories\n",
    "    # HIT-UAV structure: normal/images/, normal/labels/, or train/images, etc.\n",
    "    possible_paths = [\n",
    "        (data_root / \"normal\" / \"images\", data_root / \"normal\" / \"labels\"),\n",
    "        (data_root / \"train\" / \"images\", data_root / \"train\" / \"labels\"),\n",
    "        (data_root / \"images\", data_root / \"labels\"),\n",
    "    ]\n",
    "    \n",
    "    images_dir = None\n",
    "    labels_dir = None\n",
    "    \n",
    "    for img_dir, lbl_dir in possible_paths:\n",
    "        if img_dir.exists():\n",
    "            images_dir = img_dir\n",
    "            labels_dir = lbl_dir\n",
    "            print(f\"Found images at: {images_dir}\")\n",
    "            print(f\"Found labels at: {labels_dir}\")\n",
    "            break\n",
    "    \n",
    "    if images_dir is None:\n",
    "        # List what we have\n",
    "        print(\"ERROR: Could not find standard structure. Contents:\")\n",
    "        for item in data_root.iterdir():\n",
    "            print(f\"  {item}\")\n",
    "        raise FileNotFoundError(f\"Cannot find images directory in {data_root}\")\n",
    "    \n",
    "    # COCO format\n",
    "    coco = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": [{\"id\": 1, \"name\": \"person\", \"supercategory\": \"human\"}]\n",
    "    }\n",
    "    \n",
    "    PERSON_CLASS = 0  # HIT-UAV person class\n",
    "    ann_id = 1\n",
    "    img_id = 1\n",
    "    \n",
    "    # Get all image files\n",
    "    image_files = sorted(list(images_dir.glob(\"*.jpg\")) + list(images_dir.glob(\"*.png\")))\n",
    "    print(f\"\\nProcessing {len(image_files)} images...\")\n",
    "    \n",
    "    images_with_persons = 0\n",
    "    images_without_persons = 0\n",
    "    images_without_labels = 0\n",
    "    total_persons = 0\n",
    "    skipped_tiny_boxes = 0\n",
    "    \n",
    "    for img_path in tqdm(image_files, desc=\"Converting\"):\n",
    "        # Read image to get dimensions\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "            continue\n",
    "        \n",
    "        orig_h, orig_w = img.shape[:2]\n",
    "        \n",
    "        # Look for corresponding label file\n",
    "        label_path = labels_dir / f\"{img_path.stem}.txt\"\n",
    "        \n",
    "        person_annotations = []\n",
    "        \n",
    "        if not label_path.exists():\n",
    "            images_without_labels += 1\n",
    "            continue\n",
    "        \n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    cls_id = int(parts[0])\n",
    "                    if cls_id == PERSON_CLASS:\n",
    "                        # YOLO format: class cx cy w h (normalized 0-1)\n",
    "                        cx, cy, bw, bh = map(float, parts[1:5])\n",
    "                        \n",
    "                        # Convert to pixel coordinates\n",
    "                        x = (cx - bw / 2) * orig_w\n",
    "                        y = (cy - bh / 2) * orig_h\n",
    "                        w = bw * orig_w\n",
    "                        h = bh * orig_h\n",
    "                        \n",
    "                        # Clip to image bounds\n",
    "                        x = max(0, x)\n",
    "                        y = max(0, y)\n",
    "                        w = min(w, orig_w - x)\n",
    "                        h = min(h, orig_h - y)\n",
    "                        \n",
    "                        if w > 5 and h > 5:  # Skip tiny boxes\n",
    "                            person_annotations.append([x, y, w, h])\n",
    "                        else:\n",
    "                            skipped_tiny_boxes += 1\n",
    "        \n",
    "        # Only include images with person annotations\n",
    "        if len(person_annotations) == 0:\n",
    "            images_without_persons += 1\n",
    "            continue\n",
    "        \n",
    "        images_with_persons += 1\n",
    "        \n",
    "        # Resize image\n",
    "        scale_x = target_size / orig_w\n",
    "        scale_y = target_size / orig_h\n",
    "        img_resized = cv2.resize(img, (target_size, target_size))\n",
    "        \n",
    "        # Save resized image\n",
    "        new_filename = f\"hituav_{img_id:05d}.jpg\"\n",
    "        cv2.imwrite(str(images_out / new_filename), img_resized)\n",
    "        \n",
    "        # Add image info\n",
    "        coco[\"images\"].append({\n",
    "            \"id\": img_id,\n",
    "            \"file_name\": new_filename,\n",
    "            \"width\": target_size,\n",
    "            \"height\": target_size,\n",
    "            \"original_file\": img_path.name\n",
    "        })\n",
    "        \n",
    "        # Add scaled annotations\n",
    "        for (x, y, w, h) in person_annotations:\n",
    "            # Scale to new size\n",
    "            x_scaled = x * scale_x\n",
    "            y_scaled = y * scale_y\n",
    "            w_scaled = w * scale_x\n",
    "            h_scaled = h * scale_y\n",
    "            \n",
    "            # Skip if too small after scaling\n",
    "            if w_scaled < 8 or h_scaled < 8:\n",
    "                skipped_tiny_boxes += 1\n",
    "                continue\n",
    "            \n",
    "            coco[\"annotations\"].append({\n",
    "                \"id\": ann_id,\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": 1,\n",
    "                \"bbox\": [x_scaled, y_scaled, w_scaled, h_scaled],  # COCO format: x, y, w, h\n",
    "                \"area\": w_scaled * h_scaled,\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "            ann_id += 1\n",
    "            total_persons += 1\n",
    "        \n",
    "        img_id += 1\n",
    "    \n",
    "    # Save annotations\n",
    "    with open(ann_path, 'w') as f:\n",
    "        json.dump(coco, f)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Conversion complete:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Total images processed: {len(image_files)}\")\n",
    "    print(f\"  Images with persons: {images_with_persons}\")\n",
    "    print(f\"  Images without persons: {images_without_persons}\")\n",
    "    print(f\"  Images without labels: {images_without_labels}\")\n",
    "    print(f\"  Total person annotations: {total_persons}\")\n",
    "    print(f\"  Skipped tiny boxes: {skipped_tiny_boxes}\")\n",
    "    print(f\"  Avg persons per image: {total_persons/max(images_with_persons,1):.1f}\")\n",
    "    print(f\"  Saved to: {output_path}\")\n",
    "    \n",
    "    if images_with_persons == 0:\n",
    "        print(f\"\\n{'!'*70}\")\n",
    "        print(f\"WARNING: NO IMAGES WITH PERSON ANNOTATIONS FOUND!\")\n",
    "        print(f\"This likely means:\")\n",
    "        print(f\"  1. The labels directory is empty or missing\")\n",
    "        print(f\"  2. Person class ID is not 0 in your dataset\")\n",
    "        print(f\"  3. Label file format is different than expected\")\n",
    "        print(f\"{'!'*70}\")\n",
    "    \n",
    "    return images_out, ann_path\n",
    "\n",
    "\n",
    "# Convert dataset\n",
    "print(\"Preparing dataset (prefer YOLO labels)...\")\n",
    "YOLO_IMAGES_DIR, YOLO_LABELS_DIR = find_hituav_yolo(Config.DATA_ROOT)\n",
    "\n",
    "if Config.USE_YOLO_DIRECT and YOLO_IMAGES_DIR and YOLO_LABELS_DIR:\n",
    "    print(\"YOLO-format labels detected; skipping COCO conversion and using Kaggle dataset directly.\")\n",
    "    DATA_FORMAT = 'yolo'\n",
    "    IMAGES_DIR = YOLO_IMAGES_DIR\n",
    "    LABELS_DIR = YOLO_LABELS_DIR\n",
    "    ANNOTATIONS_PATH = None\n",
    "else:\n",
    "    print(\"YOLO labels not found or USE_YOLO_DIRECT=False; falling back to COCO conversion.\")\n",
    "    DATA_FORMAT = 'coco'\n",
    "    LABELS_DIR = None\n",
    "    IMAGES_DIR, ANNOTATIONS_PATH = convert_hituav_to_coco(\n",
    "        Config.DATA_ROOT,\n",
    "        Config.CURATED_ROOT,\n",
    "        target_size=Config.IMG_SIZE,\n",
    "        force_reconvert=False  # Set to True to force reconversion\n",
    "    )\n",
    "\n",
    "# Display sample images (COCO only)\n",
    "print(f\"\\n{'='*70}\")\n",
    "\n",
    "if DATA_FORMAT == 'coco':\n",
    "    print(\"Sample converted images with person annotations:\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    with open(ANNOTATIONS_PATH, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    if len(coco_data['images']) > 0:\n",
    "        sample_imgs = random.sample(coco_data['images'], min(4, len(coco_data['images'])))\n",
    "\n",
    "        # Group annotations by image\n",
    "        img_to_anns = defaultdict(list)\n",
    "        for ann in coco_data['annotations']:\n",
    "            img_to_anns[ann['image_id']].append(ann)\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for idx, img_info in enumerate(sample_imgs):\n",
    "            img_path = IMAGES_DIR / img_info['file_name']\n",
    "            img = cv2.imread(str(img_path))\n",
    "\n",
    "            if img is not None:\n",
    "                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # Draw annotations\n",
    "                anns = img_to_anns[img_info['id']]\n",
    "                for ann in anns:\n",
    "                    x, y, w, h = map(int, ann['bbox'])\n",
    "                    cv2.rectangle(img_rgb, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "                axes[idx].imshow(img_rgb)\n",
    "                axes[idx].set_title(f\"{img_info['file_name']}\\n{len(anns)} person(s)\")\n",
    "                axes[idx].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{Config.OUTPUT_DIR}/converted_coco_samples.png\", dpi=150)\n",
    "        plt.show()\n",
    "        print(f\"Sample images saved to: {Config.OUTPUT_DIR}/converted_coco_samples.png\")\n",
    "    else:\n",
    "        print(\"ERROR: No images in converted dataset!\")\n",
    "else:\n",
    "    print(\"Skipping COCO visualization because YOLO labels are used directly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: SAR Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: SAR AUGMENTATIONS\n",
    "# Snow, Smoke/Fire, Thermal artifacts for robustness\n",
    "# =============================================================================\n",
    "\n",
    "class SARaugmentations:\n",
    "    \"\"\"Realistic augmentations for SAR drone imagery.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_perlin_noise(shape: Tuple[int, int], scale: float = 100.0) -> np.ndarray:\n",
    "        \"\"\"Generate Perlin-like noise using octaves of Gaussian noise.\"\"\"\n",
    "        h, w = shape\n",
    "        noise = np.zeros((h, w), dtype=np.float32)\n",
    "        \n",
    "        for octave in range(4):\n",
    "            freq = 2 ** octave\n",
    "            amplitude = 1.0 / freq\n",
    "            small_h = max(2, int(h / (scale / freq)))\n",
    "            small_w = max(2, int(w / (scale / freq)))\n",
    "            small_noise = np.random.randn(small_h, small_w).astype(np.float32)\n",
    "            upscaled = cv2.resize(small_noise, (w, h), interpolation=cv2.INTER_CUBIC)\n",
    "            noise += amplitude * upscaled\n",
    "        \n",
    "        noise = (noise - noise.min()) / (noise.max() - noise.min() + 1e-8)\n",
    "        return noise\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_snow(img: np.ndarray, intensity: float = 0.5) -> np.ndarray:\n",
    "        \"\"\"Apply realistic snow effect.\"\"\"\n",
    "        h, w = img.shape[:2]\n",
    "        is_color = len(img.shape) == 3\n",
    "        \n",
    "        snow_noise = SARaugmentations.generate_perlin_noise((h, w), scale=50.0)\n",
    "        fine_noise = np.random.rand(h, w).astype(np.float32)\n",
    "        fine_noise = cv2.GaussianBlur(fine_noise, (5, 5), 0)\n",
    "        \n",
    "        snow_layer = 0.6 * snow_noise + 0.4 * fine_noise\n",
    "        snow_layer = np.clip(snow_layer * intensity * 255, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        if is_color:\n",
    "            snow_layer = cv2.cvtColor(snow_layer, cv2.COLOR_GRAY2BGR)\n",
    "        \n",
    "        img_float = img.astype(np.float32)\n",
    "        mean_val = np.mean(img_float)\n",
    "        contrast_reduction = 0.3\n",
    "        img_reduced = (1 - contrast_reduction) * img_float + contrast_reduction * mean_val\n",
    "        \n",
    "        alpha = intensity * 0.7\n",
    "        result = (1 - alpha) * img_reduced + alpha * snow_layer.astype(np.float32)\n",
    "        return np.clip(result, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_smoke_fire(img: np.ndarray, smoke_intensity: float = 0.4, \n",
    "                         fire_intensity: float = 0.3) -> np.ndarray:\n",
    "        \"\"\"Apply smoke and fire effects.\"\"\"\n",
    "        h, w = img.shape[:2]\n",
    "        if len(img.shape) != 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "        \n",
    "        result = img.astype(np.float32)\n",
    "        \n",
    "        # Smoke\n",
    "        smoke_noise = SARaugmentations.generate_perlin_noise((h, w), scale=80.0)\n",
    "        gradient = np.linspace(1.0, 0.3, h).reshape(-1, 1)\n",
    "        gradient = np.tile(gradient, (1, w))\n",
    "        smoke_mask = smoke_noise * gradient\n",
    "        \n",
    "        smoke_color = np.array([180, 180, 180], dtype=np.float32)\n",
    "        smoke_layer = np.ones((h, w, 3), dtype=np.float32) * smoke_color\n",
    "        smoke_layer = cv2.GaussianBlur(smoke_layer, (21, 21), 0)\n",
    "        \n",
    "        smoke_alpha = smoke_mask[..., np.newaxis] * smoke_intensity\n",
    "        result = result * (1 - smoke_alpha) + smoke_layer * smoke_alpha\n",
    "        \n",
    "        # Fire\n",
    "        if fire_intensity > 0:\n",
    "            fx, fy = np.random.randint(w//4, 3*w//4), np.random.randint(h//2, h)\n",
    "            y_coords, x_coords = np.ogrid[:h, :w]\n",
    "            dist = np.sqrt((x_coords - fx)**2 + (y_coords - fy)**2)\n",
    "            fire_radius = min(h, w) // 3\n",
    "            fire_mask = np.clip(1 - dist / fire_radius, 0, 1) ** 2\n",
    "            \n",
    "            fire_color = np.array([30, 100, 255], dtype=np.float32)  # Orange BGR\n",
    "            fire_layer = np.ones((h, w, 3), dtype=np.float32) * fire_color\n",
    "            \n",
    "            fire_alpha = fire_mask[..., np.newaxis] * fire_intensity\n",
    "            result = result * (1 - fire_alpha) + fire_layer * fire_alpha\n",
    "        \n",
    "        return np.clip(result, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_thermal_artifacts(img: np.ndarray, intensity_scale: float = 1.0,\n",
    "                                sensor_noise: float = 0.05) -> np.ndarray:\n",
    "        \"\"\"Apply thermal camera artifacts.\"\"\"\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        if len(img.shape) == 3:\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray = img.copy()\n",
    "        \n",
    "        result = gray.astype(np.float32) * intensity_scale\n",
    "        \n",
    "        # Sensor noise\n",
    "        noise = np.random.normal(0, sensor_noise * 255, (h, w)).astype(np.float32)\n",
    "        if np.random.rand() < 0.3:\n",
    "            line_noise = np.random.normal(0, sensor_noise * 50, (h, 1))\n",
    "            noise += np.tile(line_noise, (1, w))\n",
    "        \n",
    "        result += noise\n",
    "        result = np.clip(result, 0, 255).astype(np.uint8)\n",
    "        return cv2.cvtColor(result, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_random(img: np.ndarray) -> Tuple[np.ndarray, str]:\n",
    "        \"\"\"Apply random SAR augmentation.\"\"\"\n",
    "        aug_type = np.random.choice(['snow', 'fire', 'thermal', 'none'])\n",
    "        \n",
    "        if aug_type == 'snow':\n",
    "            return SARaugmentations.apply_snow(img, np.random.uniform(0.3, 0.6)), 'snow'\n",
    "        elif aug_type == 'fire':\n",
    "            return SARaugmentations.apply_smoke_fire(img, np.random.uniform(0.2, 0.4),\n",
    "                                                     np.random.uniform(0.2, 0.4)), 'fire'\n",
    "        elif aug_type == 'thermal':\n",
    "            return SARaugmentations.apply_thermal_artifacts(img, np.random.uniform(0.8, 1.2),\n",
    "                                                            np.random.uniform(0.03, 0.08)), 'thermal'\n",
    "        return img, 'none'\n",
    "\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(\"Visualizing SAR augmentations on actual dataset images\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Use actual dataset images for visualization\n",
    "sample_imgs = list(IMAGES_DIR.glob(\"*.jpg\"))\n",
    "\n",
    "if len(sample_imgs) > 0:\n",
    "    # Get 4 random samples from the dataset\n",
    "    samples_to_show = random.sample(sample_imgs, min(4, len(sample_imgs)))\n",
    "    \n",
    "    print(f\"\\nShowing {len(samples_to_show)} random images from the dataset:\")\n",
    "    for img_path in samples_to_show:\n",
    "        print(f\"  - {img_path.name}\")\n",
    "    \n",
    "    fig, axes = plt.subplots(len(samples_to_show), 4, figsize=(16, 4 * len(samples_to_show)))\n",
    "    if len(samples_to_show) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for row_idx, img_path in enumerate(samples_to_show):\n",
    "        sample = cv2.imread(str(img_path))\n",
    "        if sample is None:\n",
    "            continue\n",
    "        \n",
    "        # Original\n",
    "        axes[row_idx, 0].imshow(cv2.cvtColor(sample, cv2.COLOR_BGR2RGB))\n",
    "        axes[row_idx, 0].set_title(f'Original\\n{img_path.name}')\n",
    "        axes[row_idx, 0].axis('off')\n",
    "        \n",
    "        # Snow\n",
    "        snow_aug = SARaugmentations.apply_snow(sample, 0.5)\n",
    "        axes[row_idx, 1].imshow(cv2.cvtColor(snow_aug, cv2.COLOR_BGR2RGB))\n",
    "        axes[row_idx, 1].set_title('Snow')\n",
    "        axes[row_idx, 1].axis('off')\n",
    "        \n",
    "        # Smoke/Fire\n",
    "        fire_aug = SARaugmentations.apply_smoke_fire(sample, 0.4, 0.4)\n",
    "        axes[row_idx, 2].imshow(cv2.cvtColor(fire_aug, cv2.COLOR_BGR2RGB))\n",
    "        axes[row_idx, 2].set_title('Smoke/Fire')\n",
    "        axes[row_idx, 2].axis('off')\n",
    "        \n",
    "        # Thermal Artifacts\n",
    "        thermal_aug = SARaugmentations.apply_thermal_artifacts(sample, 1.1, 0.06)\n",
    "        axes[row_idx, 3].imshow(cv2.cvtColor(thermal_aug, cv2.COLOR_BGR2RGB))\n",
    "        axes[row_idx, 3].set_title('Thermal Artifacts')\n",
    "        axes[row_idx, 3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{Config.OUTPUT_DIR}/augmentations_dataset_samples.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"\\nAugmentations saved to: {Config.OUTPUT_DIR}/augmentations_dataset_samples.png\")\n",
    "    \n",
    "    # Test random augmentation\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Testing apply_random() on dataset images:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx in range(6):\n",
    "        sample_img = cv2.imread(str(random.choice(sample_imgs)))\n",
    "        if sample_img is not None:\n",
    "            aug_img, aug_type = SARaugmentations.apply_random(sample_img)\n",
    "            axes[idx].imshow(cv2.cvtColor(aug_img, cv2.COLOR_BGR2RGB))\n",
    "            axes[idx].set_title(f'Random Aug: {aug_type}')\n",
    "            axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{Config.OUTPUT_DIR}/augmentations_random_samples.png\", dpi=150)\n",
    "    plt.show()\n",
    "    print(f\"Random augmentations saved to: {Config.OUTPUT_DIR}/augmentations_random_samples.png\")\n",
    "    \n",
    "else:\n",
    "    print(\"WARNING: No images found in IMAGES_DIR. Cannot visualize augmentations.\")\n",
    "    print(f\"IMAGES_DIR: {IMAGES_DIR}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SAR augmentations defined and tested successfully!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Dataset Class with Proper Box Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# CELL 6: DATASET CLASS\n",
    "# Proper handling of box formats to avoid evaluation bugs\n",
    "# Supports both COCO (converted) and native YOLO .txt labels\n",
    "# =============================================================================\n",
    "\n",
    "class UAVDetectionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for UAV person detection with proper box format handling.\n",
    "    - COCO mode: uses annotations.json generated during conversion\n",
    "    - YOLO mode: reads .txt labels on-the-fly and keeps original images\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, images_dir, annotations_path=None, labels_dir=None, transforms=None,\n",
    "                 apply_sar_aug=False, sar_aug_prob=0.5, selected_files=None):\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.labels_dir = Path(labels_dir) if labels_dir else None\n",
    "        self.transforms = transforms\n",
    "        self.apply_sar_aug = apply_sar_aug\n",
    "        self.sar_aug_prob = sar_aug_prob\n",
    "        self.format = 'coco' if annotations_path is not None else 'yolo'\n",
    "        self.selected_files = selected_files\n",
    "\n",
    "        if self.format == 'coco':\n",
    "            with open(annotations_path, 'r') as f:\n",
    "                coco = json.load(f)\n",
    "            self.images = {img['id']: img for img in coco['images']}\n",
    "            # Group annotations by image\n",
    "            self.img_to_anns = defaultdict(list)\n",
    "            for ann in coco['annotations']:\n",
    "                self.img_to_anns[ann['image_id']].append(ann)\n",
    "            # Only keep images WITH annotations\n",
    "            self.img_ids = [img_id for img_id in self.images.keys()\n",
    "                            if len(self.img_to_anns[img_id]) > 0]\n",
    "            print(f\"Dataset initialized (COCO): {len(self.img_ids)} images with annotations\")\n",
    "        else:\n",
    "            # YOLO: gather image files, optionally subset\n",
    "            candidate_files = self.selected_files if self.selected_files is not None else (\n",
    "                list(self.images_dir.glob('*.jpg')) + list(self.images_dir.glob('*.png'))\n",
    "            )\n",
    "            self.image_files = [Path(p) for p in candidate_files\n",
    "                                if (self.labels_dir / f\"{Path(p).stem}.txt\").exists()]\n",
    "            self.img_ids = list(range(len(self.image_files)))\n",
    "            print(f\"Dataset initialized (YOLO): {len(self.img_ids)} images with labels\")\n",
    "\n",
    "        print(f\"  - Images dir: {self.images_dir}\")\n",
    "        if self.labels_dir:\n",
    "            print(f\"  - Labels dir:  {self.labels_dir}\")\n",
    "        print(f\"  - SAR augmentation: {'Enabled' if self.apply_sar_aug else 'Disabled'}\")\n",
    "        if self.apply_sar_aug:\n",
    "            print(f\"  - SAR aug probability: {self.sar_aug_prob}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.format == 'coco':\n",
    "            img_id = self.img_ids[idx]\n",
    "            img_info = self.images[img_id]\n",
    "            img_path = self.images_dir / img_info['file_name']\n",
    "            img = cv2.imread(str(img_path))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            anns = self.img_to_anns[img_id]\n",
    "        else:\n",
    "            img_path = self.image_files[idx]\n",
    "            img = cv2.imread(str(img_path))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img_id = idx  # simple monotonically increasing id\n",
    "            label_path = self.labels_dir / f\"{img_path.stem}.txt\"\n",
    "            anns = []\n",
    "            h, w = img.shape[:2]\n",
    "            if label_path.exists():\n",
    "                with open(label_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) >= 5:\n",
    "                            cls_id = int(parts[0])\n",
    "                            if cls_id == 0:  # person\n",
    "                                cx, cy, bw, bh = map(float, parts[1:5])\n",
    "                                x = (cx - bw / 2) * w\n",
    "                                y = (cy - bh / 2) * h\n",
    "                                bw_pix = bw * w\n",
    "                                bh_pix = bh * h\n",
    "                                anns.append({'bbox': [x, y, bw_pix, bh_pix], 'area': bw_pix * bh_pix})\n",
    "\n",
    "        # Apply SAR augmentation\n",
    "        if self.apply_sar_aug and random.random() < self.sar_aug_prob:\n",
    "            img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "            img_aug, _ = SARaugmentations.apply_random(img_bgr)\n",
    "            img = cv2.cvtColor(img_aug, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        areas = []\n",
    "\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            x1 = x\n",
    "            y1 = y\n",
    "            x2 = x + w\n",
    "            y2 = y + h\n",
    "            if x2 > x1 and y2 > y1:\n",
    "                boxes.append([x1, y1, x2, y2])\n",
    "                labels.append(1)  # person class\n",
    "                areas.append(ann.get('area', w * h))\n",
    "\n",
    "        img_tensor = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
    "        if self.transforms:\n",
    "            img_tensor = self.transforms(img_tensor)\n",
    "\n",
    "        if len(boxes) > 0:\n",
    "            boxes_tensor = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels_tensor = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            areas_tensor = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        else:\n",
    "            boxes_tensor = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels_tensor = torch.zeros((0,), dtype=torch.int64)\n",
    "            areas_tensor = torch.zeros((0,), dtype=torch.float32)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes_tensor,\n",
    "            'labels': labels_tensor,\n",
    "            'image_id': torch.tensor([img_id]),\n",
    "            'area': areas_tensor,\n",
    "            'iscrowd': torch.zeros(len(boxes), dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "        return img_tensor, target\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate for detection.\"\"\"\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "# Create train/val/test split (70/15/15)\n",
    "print(f\"\n",
    "{'='*70}\")\n",
    "print(\"Creating train/val/test splits (70/15/15)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "TRAIN_ANN = VAL_ANN = TEST_ANN = None\n",
    "TRAIN_FILES = VAL_FILES = TEST_FILES = None\n",
    "\n",
    "if DATA_FORMAT == 'coco':\n",
    "    with open(ANNOTATIONS_PATH, 'r') as f:\n",
    "        full_coco = json.load(f)\n",
    "    all_images = full_coco['images'].copy()\n",
    "    random.shuffle(all_images)\n",
    "    n = len(all_images)\n",
    "    train_end = int(0.7 * n)\n",
    "    val_end = int(0.85 * n)\n",
    "    train_images = all_images[:train_end]\n",
    "    val_images = all_images[train_end:val_end]\n",
    "    test_images = all_images[val_end:]\n",
    "    train_ids = set(img['id'] for img in train_images)\n",
    "    val_ids = set(img['id'] for img in val_images)\n",
    "    test_ids = set(img['id'] for img in test_images)\n",
    "    train_anns = [a for a in full_coco['annotations'] if a['image_id'] in train_ids]\n",
    "    val_anns = [a for a in full_coco['annotations'] if a['image_id'] in val_ids]\n",
    "    test_anns = [a for a in full_coco['annotations'] if a['image_id'] in test_ids]\n",
    "    splits = {\n",
    "        'train': (train_images, train_anns),\n",
    "        'val': (val_images, val_anns),\n",
    "        'test': (test_images, test_anns)\n",
    "    }\n",
    "    for split_name, (images, anns) in splits.items():\n",
    "        split_coco = {\n",
    "            'images': images,\n",
    "            'annotations': anns,\n",
    "            'categories': full_coco['categories']\n",
    "        }\n",
    "        split_path = Path(Config.CURATED_ROOT) / f\"{split_name}.json\"\n",
    "        with open(split_path, 'w') as f:\n",
    "            json.dump(split_coco, f)\n",
    "        print(f\"{split_name}: {len(images)} images, {len(anns)} annotations\")\n",
    "    TRAIN_ANN = Path(Config.CURATED_ROOT) / \"train.json\"\n",
    "    VAL_ANN = Path(Config.CURATED_ROOT) / \"val.json\"\n",
    "    TEST_ANN = Path(Config.CURATED_ROOT) / \"test.json\"\n",
    "else:\n",
    "    # YOLO split using existing label files\n",
    "    all_images = sorted(list(IMAGES_DIR.glob('*.jpg')) + list(IMAGES_DIR.glob('*.png')))\n",
    "    all_images = [p for p in all_images if (LABELS_DIR / f\"{p.stem}.txt\").exists()]\n",
    "    random.shuffle(all_images)\n",
    "    n = len(all_images)\n",
    "    train_end = int(0.7 * n)\n",
    "    val_end = int(0.85 * n)\n",
    "    TRAIN_FILES = all_images[:train_end]\n",
    "    VAL_FILES = all_images[train_end:val_end]\n",
    "    TEST_FILES = all_images[val_end:]\n",
    "    print(f\"train: {len(TRAIN_FILES)} images\")\n",
    "    print(f\"val:   {len(VAL_FILES)} images\")\n",
    "    print(f\"test:  {len(TEST_FILES)} images\")\n",
    "\n",
    "# Test dataset loading with visualization\n",
    "print(f\"\n",
    "{'='*70}\")\n",
    "print(\"Testing dataset loading and augmentation\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "train_ds_kwargs = {\n",
    "    'images_dir': IMAGES_DIR,\n",
    "    'annotations_path': TRAIN_ANN,\n",
    "    'labels_dir': LABELS_DIR,\n",
    "    'selected_files': TRAIN_FILES,\n",
    "    'apply_sar_aug': False\n",
    "}\n",
    "\n",
    "aug_ds_kwargs = train_ds_kwargs.copy()\n",
    "aug_ds_kwargs['apply_sar_aug'] = True\n",
    "aug_ds_kwargs['sar_aug_prob'] = 1.0\n",
    "\n",
    "test_dataset_no_aug = UAVDetectionDataset(**train_ds_kwargs)\n",
    "test_dataset_with_aug = UAVDetectionDataset(**aug_ds_kwargs)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "print(\"\n",
    "Loading and visualizing samples...\")\n",
    "\n",
    "for col in range(4):\n",
    "    idx = random.randint(0, len(test_dataset_no_aug) - 1)\n",
    "    img_tensor, target = test_dataset_no_aug[idx]\n",
    "    img_np = img_tensor.permute(1, 2, 0).numpy()\n",
    "    img_np = (img_np * 255).astype(np.uint8)\n",
    "    boxes = target['boxes'].numpy()\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        cv2.rectangle(img_np, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    axes[0, col].imshow(img_np)\n",
    "    axes[0, col].set_title(f'Original\\n{len(boxes)} person(s)')\n",
    "    axes[0, col].axis('off')\n",
    "\n",
    "for col in range(4):\n",
    "    idx = random.randint(0, len(test_dataset_with_aug) - 1)\n",
    "    img_tensor, target = test_dataset_with_aug[idx]\n",
    "    img_np = img_tensor.permute(1, 2, 0).numpy()\n",
    "    img_np = (img_np * 255).astype(np.uint8)\n",
    "    boxes = target['boxes'].numpy()\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        cv2.rectangle(img_np, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    axes[1, col].imshow(img_np)\n",
    "    axes[1, col].set_title(f'Augmented\\n{len(boxes)} person(s)')\n",
    "    axes[1, col].axis('off')\n",
    "\n",
    "plt.suptitle('Dataset Loading Test: Original (top) vs Augmented (bottom)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{Config.OUTPUT_DIR}/dataset_loading_test.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\n",
    "Dataset loading test saved to: {Config.OUTPUT_DIR}/dataset_loading_test.png\")\n",
    "print(f\"{'='*70}\")\n",
    "print(\"Dataset class and splits created successfully!\")\n",
    "print(f\"{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Corrected Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 7: CORRECTED EVALUATION FUNCTION\n",
    "# Properly computes precision, recall, F1 with detailed debugging\n",
    "# =============================================================================\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Compute IoU between two boxes in [x1, y1, x2, y2] format.\n",
    "    \"\"\"\n",
    "    # Intersection\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    inter_w = max(0, x2 - x1)\n",
    "    inter_h = max(0, y2 - y1)\n",
    "    inter_area = inter_w * inter_h\n",
    "    \n",
    "    # Union\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union_area = area1 + area2 - inter_area\n",
    "    \n",
    "    if union_area <= 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return inter_area / union_area\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, data_loader, device, iou_thresh=0.5, conf_thresh=0.5, verbose=False):\n",
    "    \"\"\"\n",
    "    Evaluate detection model with proper metric computation.\n",
    "    \n",
    "    Returns precision, recall, F1 at given IoU and confidence thresholds.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    total_tp = 0\n",
    "    total_fp = 0\n",
    "    total_fn = 0\n",
    "    total_gt = 0\n",
    "    total_pred = 0\n",
    "    \n",
    "    for batch_idx, (images, targets) in enumerate(tqdm(data_loader, desc=\"Evaluating\", disable=not verbose)):\n",
    "        images = [img.to(device) for img in images]\n",
    "        \n",
    "        # Get predictions\n",
    "        outputs = model(images)\n",
    "        \n",
    "        for output, target in zip(outputs, targets):\n",
    "            # Get ground truth boxes (already in [x1, y1, x2, y2] format from dataset)\n",
    "            gt_boxes = target['boxes'].cpu().numpy()\n",
    "            total_gt += len(gt_boxes)\n",
    "            \n",
    "            # Filter predictions by confidence AND class (person = 1)\n",
    "            scores = output['scores'].cpu().numpy()\n",
    "            pred_boxes = output['boxes'].cpu().numpy()\n",
    "            pred_labels = output['labels'].cpu().numpy()\n",
    "            \n",
    "            # Only keep person predictions above threshold\n",
    "            mask = (scores >= conf_thresh) & (pred_labels == 1)\n",
    "            pred_boxes = pred_boxes[mask]\n",
    "            pred_scores = scores[mask]\n",
    "            total_pred += len(pred_boxes)\n",
    "            \n",
    "            if len(gt_boxes) == 0:\n",
    "                # All predictions are false positives\n",
    "                total_fp += len(pred_boxes)\n",
    "                continue\n",
    "            \n",
    "            if len(pred_boxes) == 0:\n",
    "                # All ground truths are missed\n",
    "                total_fn += len(gt_boxes)\n",
    "                continue\n",
    "            \n",
    "            # Match predictions to ground truth (greedy matching by score)\n",
    "            # Sort predictions by score (descending)\n",
    "            sorted_indices = np.argsort(-pred_scores)\n",
    "            matched_gt = set()\n",
    "            \n",
    "            for pred_idx in sorted_indices:\n",
    "                pred_box = pred_boxes[pred_idx]\n",
    "                \n",
    "                best_iou = 0\n",
    "                best_gt_idx = -1\n",
    "                \n",
    "                for gt_idx, gt_box in enumerate(gt_boxes):\n",
    "                    if gt_idx in matched_gt:\n",
    "                        continue\n",
    "                    \n",
    "                    iou = compute_iou(pred_box, gt_box)\n",
    "                    if iou > best_iou:\n",
    "                        best_iou = iou\n",
    "                        best_gt_idx = gt_idx\n",
    "                \n",
    "                if best_iou >= iou_thresh and best_gt_idx >= 0:\n",
    "                    total_tp += 1\n",
    "                    matched_gt.add(best_gt_idx)\n",
    "                else:\n",
    "                    total_fp += 1\n",
    "            \n",
    "            # Unmatched ground truths are false negatives\n",
    "            total_fn += len(gt_boxes) - len(matched_gt)\n",
    "    \n",
    "    # Compute metrics\n",
    "    precision = total_tp / max(total_tp + total_fp, 1)\n",
    "    recall = total_tp / max(total_tp + total_fn, 1)\n",
    "    f1 = 2 * precision * recall / max(precision + recall, 1e-8)\n",
    "    \n",
    "    metrics = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'tp': total_tp,\n",
    "        'fp': total_fp,\n",
    "        'fn': total_fn,\n",
    "        'total_gt': total_gt,\n",
    "        'total_pred': total_pred\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nEvaluation Results (IoU={iou_thresh}, Conf={conf_thresh}):\")\n",
    "        print(f\"  GT boxes: {total_gt}, Predictions: {total_pred}\")\n",
    "        print(f\"  TP: {total_tp}, FP: {total_fp}, FN: {total_fn}\")\n",
    "        print(f\"  Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "print(\"Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Model Creation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: MODEL CREATION\n",
    "# =============================================================================\n",
    "\n",
    "def create_detection_model(num_classes=2, pretrained=True, freeze_backbone=True):\n",
    "    \"\"\"\n",
    "    Create Faster R-CNN model for person detection.\n",
    "    \n",
    "    Args:\n",
    "        num_classes: 2 (background + person)\n",
    "        pretrained: Use COCO pretrained weights\n",
    "        freeze_backbone: Freeze early layers to prevent overfitting\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "        model = fasterrcnn_resnet50_fpn(weights=weights)\n",
    "    else:\n",
    "        model = fasterrcnn_resnet50_fpn(weights=None)\n",
    "    \n",
    "    # Replace the classifier head\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    if freeze_backbone:\n",
    "        # Freeze backbone except layer4 and FPN\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'backbone' in name:\n",
    "                if 'layer4' not in name and 'fpn' not in name:\n",
    "                    param.requires_grad = False\n",
    "    \n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model: {trainable:,} / {total:,} trainable params ({100*trainable/total:.1f}%)\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# YOLOV8 ALTERNATIVE:\n",
    "# from ultralytics import YOLO\n",
    "# model = YOLO('yolov8n.pt')\n",
    "# model.train(data='data.yaml', epochs=6, imgsz=512)\n",
    "# ============================================\n",
    "\n",
    "print(\"Model creation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 9: TRAINING FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs, lr,\n",
    "                checkpoint_prefix=\"model\", lr_step=3, lr_gamma=0.1):\n",
    "    \"\"\"\n",
    "    Train the detection model and track metrics.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    \n",
    "    # Optimizer - only train unfrozen params\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=lr, momentum=0.9, weight_decay=Config.WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_step, gamma=lr_gamma)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_precision': [],\n",
    "        'val_recall': [],\n",
    "        'val_f1': []\n",
    "    }\n",
    "    \n",
    "    best_f1 = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for images, targets in pbar:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Skip batches with no valid targets\n",
    "            valid_targets = [t for t in targets if len(t['boxes']) > 0]\n",
    "            if len(valid_targets) == 0:\n",
    "                continue\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            losses.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(params, max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += losses.item()\n",
    "            num_batches += 1\n",
    "            pbar.set_postfix({'loss': f\"{losses.item():.4f}\"})\n",
    "        \n",
    "        avg_loss = epoch_loss / max(num_batches, 1)\n",
    "        history['train_loss'].append(avg_loss)\n",
    "        \n",
    "        # Validation\n",
    "        metrics = evaluate_model(model, val_loader, device,\n",
    "                                 iou_thresh=Config.IOU_THRESHOLD,\n",
    "                                 conf_thresh=Config.CONF_THRESHOLD)\n",
    "        \n",
    "        history['val_precision'].append(metrics['precision'])\n",
    "        history['val_recall'].append(metrics['recall'])\n",
    "        history['val_f1'].append(metrics['f1'])\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, \"\n",
    "              f\"P={metrics['precision']:.4f}, R={metrics['recall']:.4f}, F1={metrics['f1']:.4f} \"\n",
    "              f\"(TP={metrics['tp']}, FP={metrics['fp']}, FN={metrics['fn']})\")\n",
    "        \n",
    "        # Save best model\n",
    "        if metrics['f1'] > best_f1:\n",
    "            best_f1 = metrics['f1']\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'metrics': metrics,\n",
    "                'history': history\n",
    "            }, f\"{Config.CHECKPOINT_DIR}/{checkpoint_prefix}_best.pth\")\n",
    "            print(f\"  -> Saved best model (F1={best_f1:.4f})\")\n",
    "    \n",
    "    return model, history, best_f1\n",
    "\n",
    "\n",
    "print(\"Training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Train Model A (Baseline - No SAR Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 10: TRAIN MODEL A - BASELINE (NO AUGMENTATION)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING MODEL A: BASELINE (No SAR Augmentation)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create datasets WITHOUT SAR augmentation\n",
    "train_dataset_baseline = UAVDetectionDataset(\n",
    "    IMAGES_DIR, TRAIN_ANN,\n",
    "    labels_dir=LABELS_DIR,\n",
    "    selected_files=TRAIN_FILES,\n",
    "    apply_sar_aug=False\n",
    ")\n",
    "\n",
    "val_dataset = UAVDetectionDataset(\n",
    "    IMAGES_DIR, VAL_ANN,\n",
    "    labels_dir=LABELS_DIR,\n",
    "    selected_files=VAL_FILES,\n",
    "    apply_sar_aug=False\n",
    ")\n",
    "\n",
    "train_loader_baseline = DataLoader(\n",
    "    train_dataset_baseline,\n",
    "    batch_size=Config.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=Config.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Create and train model A\n",
    "model_A = create_detection_model(\n",
    "    num_classes=Config.NUM_CLASSES,\n",
    "    pretrained=True,\n",
    "    freeze_backbone=True\n",
    ")\n",
    "\n",
    "model_A, history_A, best_f1_A = train_model(\n",
    "    model_A, train_loader_baseline, val_loader, Config.DEVICE,\n",
    "    num_epochs=Config.NUM_EPOCHS,\n",
    "    lr=Config.LR,\n",
    "    checkpoint_prefix=\"model_A_baseline\",\n",
    "    lr_step=Config.LR_STEP_SIZE,\n",
    "    lr_gamma=Config.LR_GAMMA\n",
    ")\n",
    "\n",
    "print(f\"\\nModel A Best F1: {best_f1_A:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Train Model B (With SAR Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# CELL 11: TRAIN MODEL B - WITH SAR AUGMENTATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING MODEL B: WITH SAR AUGMENTATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if Config.ENABLE_SYNTH_AUG:\n",
    "    train_dataset_augmented = UAVDetectionDataset(\n",
    "        IMAGES_DIR, TRAIN_ANN,\n",
    "        labels_dir=LABELS_DIR,\n",
    "        selected_files=TRAIN_FILES,\n",
    "        apply_sar_aug=True,\n",
    "        sar_aug_prob=0.5  # 50% chance of augmentation\n",
    "    )\n",
    "\n",
    "    train_loader_augmented = DataLoader(\n",
    "        train_dataset_augmented,\n",
    "        batch_size=Config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # Create and train model B (fresh model)\n",
    "    model_B = create_detection_model(\n",
    "        num_classes=Config.NUM_CLASSES,\n",
    "        pretrained=True,\n",
    "        freeze_backbone=True\n",
    "    )\n",
    "\n",
    "    model_B, history_B, best_f1_B = train_model(\n",
    "        model_B, train_loader_augmented, val_loader, Config.DEVICE,\n",
    "        num_epochs=Config.NUM_EPOCHS,\n",
    "        lr=Config.LR,\n",
    "        checkpoint_prefix=\"model_B_augmented\",\n",
    "        lr_step=Config.LR_STEP_SIZE,\n",
    "        lr_gamma=Config.LR_GAMMA\n",
    "    )\n",
    "\n",
    "    print(f\"\n",
    "Model B Best F1: {best_f1_B:.4f}\")\n",
    "else:\n",
    "    print(\"Config.ENABLE_SYNTH_AUG=False -> skipping Model B training to avoid synthetic images.\")\n",
    "    model_B = None\n",
    "    history_B = None\n",
    "    best_f1_B = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12: Create Perturbed Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# CELL 12: CREATE PERTURBED TEST SET\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Creating perturbed test set...\")\n",
    "\n",
    "if Config.ENABLE_SYNTH_AUG:\n",
    "    perturbed_dir = Path(Config.CURATED_ROOT) / \"perturbed_test\"\n",
    "    perturbed_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    with open(TEST_ANN, 'r') as f:\n",
    "        test_coco = json.load(f)\n",
    "\n",
    "    for img_info in tqdm(test_coco['images'], desc=\"Perturbing test images\"):\n",
    "        img_path = IMAGES_DIR / img_info['file_name']\n",
    "        img = cv2.imread(str(img_path))\n",
    "\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        # Apply random perturbation (snow or fire)\n",
    "        aug_type = random.choice(['snow', 'fire'])\n",
    "        if aug_type == 'snow':\n",
    "            perturbed = SARaugmentations.apply_snow(img, random.uniform(0.4, 0.6))\n",
    "        else:\n",
    "            perturbed = SARaugmentations.apply_smoke_fire(img, random.uniform(0.3, 0.5),\n",
    "                                                           random.uniform(0.3, 0.5))\n",
    "\n",
    "        cv2.imwrite(str(perturbed_dir / img_info['file_name']), perturbed)\n",
    "\n",
    "    # Save annotations (same as test)\n",
    "    PERTURBED_TEST_ANN = perturbed_dir / \"annotations.json\"\n",
    "    with open(PERTURBED_TEST_ANN, 'w') as f:\n",
    "        json.dump(test_coco, f)\n",
    "\n",
    "    print(f\"Perturbed test set saved to {perturbed_dir}\")\n",
    "else:\n",
    "    print(\"Synthetic perturbations disabled; skipping perturbed test set creation.\")\n",
    "    PERTURBED_TEST_ANN = None\n",
    "    perturbed_dir = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 13: Final Comparison - Both Models on Clean & Perturbed Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# CELL 13: FINAL COMPARISON\n",
    "# Evaluate models on clean (and optionally perturbed) test sets\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Always evaluate Model A on clean test set\n",
    "print(\"Creating test dataset (clean)...\")\n",
    "test_dataset_clean = UAVDetectionDataset(IMAGES_DIR, TEST_ANN,\n",
    "                                         labels_dir=LABELS_DIR,\n",
    "                                         selected_files=TEST_FILES,\n",
    "                                         apply_sar_aug=False)\n",
    "\n",
    "test_loader_clean = DataLoader(test_dataset_clean, batch_size=Config.BATCH_SIZE,\n",
    "                               shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "metrics_A_clean = evaluate_model(model_A, test_loader_clean, Config.DEVICE,\n",
    "                                 iou_thresh=Config.IOU_THRESHOLD,\n",
    "                                 conf_thresh=Config.CONF_THRESHOLD, verbose=True)\n",
    "\n",
    "print(f\"\n",
    "Model A (Baseline) on CLEAN test set -> P={metrics_A_clean['precision']:.4f}, R={metrics_A_clean['recall']:.4f}, F1={metrics_A_clean['f1']:.4f}\")\n",
    "\n",
    "# Optional: evaluate augmented model and perturbed set when enabled\n",
    "if Config.ENABLE_SYNTH_AUG and model_B is not None and PERTURBED_TEST_ANN is not None:\n",
    "    print(\"\n",
    "Creating perturbed test dataset...\")\n",
    "    test_dataset_perturbed = UAVDetectionDataset(perturbed_dir, PERTURBED_TEST_ANN,\n",
    "                                                 labels_dir=None,\n",
    "                                                 apply_sar_aug=False)\n",
    "    test_loader_perturbed = DataLoader(test_dataset_perturbed, batch_size=Config.BATCH_SIZE,\n",
    "                                       shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "    metrics_A_perturbed = evaluate_model(model_A, test_loader_perturbed, Config.DEVICE,\n",
    "                                         iou_thresh=Config.IOU_THRESHOLD,\n",
    "                                         conf_thresh=Config.CONF_THRESHOLD, verbose=True)\n",
    "\n",
    "    print(\"\n",
    "--- Model B (Augmented) ---\")\n",
    "    print(\"On CLEAN test set:\")\n",
    "    metrics_B_clean = evaluate_model(model_B, test_loader_clean, Config.DEVICE,\n",
    "                                      iou_thresh=Config.IOU_THRESHOLD,\n",
    "                                      conf_thresh=Config.CONF_THRESHOLD, verbose=True)\n",
    "\n",
    "    print(\"\n",
    "On PERTURBED test set:\")\n",
    "    metrics_B_perturbed = evaluate_model(model_B, test_loader_perturbed, Config.DEVICE,\n",
    "                                          iou_thresh=Config.IOU_THRESHOLD,\n",
    "                                          conf_thresh=Config.CONF_THRESHOLD, verbose=True)\n",
    "\n",
    "    # Summary Table\n",
    "    print(\"\n",
    "\" + \"=\"*70)\n",
    "    print(\"SUMMARY TABLE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Model':<20} {'Test Set':<15} {'Precision':<12} {'Recall':<12} {'F1':<12}\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'A (Baseline)':<20} {'Clean':<15} {metrics_A_clean['precision']:.4f}       {metrics_A_clean['recall']:.4f}       {metrics_A_clean['f1']:.4f}\")\n",
    "    print(f\"{'A (Baseline)':<20} {'Perturbed':<15} {metrics_A_perturbed['precision']:.4f}       {metrics_A_perturbed['recall']:.4f}       {metrics_A_perturbed['f1']:.4f}\")\n",
    "    print(f\"{'B (Augmented)':<20} {'Clean':<15} {metrics_B_clean['precision']:.4f}       {metrics_B_clean['recall']:.4f}       {metrics_B_clean['f1']:.4f}\")\n",
    "    print(f\"{'B (Augmented)':<20} {'Perturbed':<15} {metrics_B_perturbed['precision']:.4f}       {metrics_B_perturbed['recall']:.4f}       {metrics_B_perturbed['f1']:.4f}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    drop_A = (metrics_A_clean['f1'] - metrics_A_perturbed['f1']) / max(metrics_A_clean['f1'], 1e-8) * 100\n",
    "    drop_B = (metrics_B_clean['f1'] - metrics_B_perturbed['f1']) / max(metrics_B_clean['f1'], 1e-8) * 100\n",
    "    print(f\"\n",
    "Robustness Analysis:\")\n",
    "    print(f\"  Model A F1 drop on perturbed: {drop_A:.1f}%\")\n",
    "    print(f\"  Model B F1 drop on perturbed: {drop_B:.1f}%\")\n",
    "    print(f\"  Robustness improvement: {drop_A - drop_B:.1f}%\")\n",
    "    if drop_B < drop_A:\n",
    "        print(\"\n",
    "-> Model B (with SAR augmentation) is MORE ROBUST to adverse conditions!\")\n",
    "    else:\n",
    "        print(\"\n",
    "-> Augmentation did not improve robustness (may need tuning)\")\n",
    "else:\n",
    "    print(\"Synthetic augmentation disabled; only Model A evaluated on clean test set.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 14: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# CELL 14: VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "if Config.ENABLE_SYNTH_AUG and history_B is not None:\n",
    "    # Training curves comparison (baseline vs augmented)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes[0, 0].plot(history_A['train_loss'], 'b-o', label='Model A (Baseline)')\n",
    "    axes[0, 0].plot(history_B['train_loss'], 'r-s', label='Model B (Augmented)')\n",
    "    axes[0, 0].set_xlabel('Epoch'); axes[0, 0].set_ylabel('Loss'); axes[0, 0].set_title('Training Loss'); axes[0, 0].legend(); axes[0, 0].grid(True)\n",
    "    axes[0, 1].plot(history_A['val_f1'], 'b-o', label='Model A (Baseline)')\n",
    "    axes[0, 1].plot(history_B['val_f1'], 'r-s', label='Model B (Augmented)')\n",
    "    axes[0, 1].set_xlabel('Epoch'); axes[0, 1].set_ylabel('F1 Score'); axes[0, 1].set_title('Validation F1'); axes[0, 1].legend(); axes[0, 1].grid(True)\n",
    "    axes[1, 0].plot(history_A['val_precision'], 'b-o', label='Model A (Baseline)')\n",
    "    axes[1, 0].plot(history_B['val_precision'], 'r-s', label='Model B (Augmented)')\n",
    "    axes[1, 0].set_xlabel('Epoch'); axes[1, 0].set_ylabel('Precision'); axes[1, 0].set_title('Validation Precision'); axes[1, 0].legend(); axes[1, 0].grid(True)\n",
    "    axes[1, 1].plot(history_A['val_recall'], 'b-o', label='Model A (Baseline)')\n",
    "    axes[1, 1].plot(history_B['val_recall'], 'r-s', label='Model B (Augmented)')\n",
    "    axes[1, 1].set_xlabel('Epoch'); axes[1, 1].set_ylabel('Recall'); axes[1, 1].set_title('Validation Recall'); axes[1, 1].legend(); axes[1, 1].grid(True)\n",
    "    plt.tight_layout(); plt.savefig(f\"{Config.OUTPUT_DIR}/training_comparison.png\", dpi=150); plt.show()\n",
    "    # Final metrics bar chart\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    x = np.arange(3); width = 0.2\n",
    "    metrics_labels = ['Precision', 'Recall', 'F1']\n",
    "    bars1 = ax.bar(x - 1.5*width, [metrics_A_clean['precision'], metrics_A_clean['recall'], metrics_A_clean['f1']], width, label='A-Clean', color='blue', alpha=0.8)\n",
    "    bars3 = ax.bar(x + 0.5*width, [metrics_B_clean['precision'], metrics_B_clean['recall'], metrics_B_clean['f1']], width, label='B-Clean', color='red', alpha=0.8)\n",
    "    ax.set_ylabel('Score'); ax.set_title('Model Comparison: Clean vs Perturbed Test Sets'); ax.set_xticks(x); ax.set_xticklabels(metrics_labels); ax.legend(); ax.set_ylim(0, 1.0); ax.grid(True, axis='y', alpha=0.3)\n",
    "    plt.tight_layout(); plt.savefig(f\"{Config.OUTPUT_DIR}/metrics_comparison.png\", dpi=150); plt.show()\n",
    "    print(f\"\n",
    "Visualizations saved to {Config.OUTPUT_DIR}/\")\n",
    "else:\n",
    "    print(\"Synthetic augmentation disabled; skipping comparison visualizations.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 15: Sample Predictions Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# CELL 15: SAMPLE PREDICTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def visualize_detections(model, images_dir, annotations_path, title, num_samples=4):\n",
    "    \"\"\"Visualize model detections on sample images.\"\"\"\n",
    "    model.eval()\n",
    "    with open(annotations_path, 'r') as f:\n",
    "        coco = json.load(f)\n",
    "    img_to_anns = defaultdict(list)\n",
    "    for ann in coco['annotations']:\n",
    "        img_to_anns[ann['image_id']].append(ann)\n",
    "    samples = random.sample(coco['images'], min(num_samples, len(coco['images'])))\n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(4*num_samples, 8))\n",
    "    for i, img_info in enumerate(samples):\n",
    "        img_path = images_dir / img_info['file_name']\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        gt_img = img_rgb.copy()\n",
    "        for ann in img_to_anns[img_info['id']]:\n",
    "            x, y, w, h = map(int, ann['bbox'])\n",
    "            cv2.rectangle(gt_img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        axes[0, i].imshow(gt_img); axes[0, i].set_title(f'GT ({len(img_to_anns[img_info['id']])})'); axes[0, i].axis('off')\n",
    "        img_tensor = torch.from_numpy(img_rgb).permute(2, 0, 1).float() / 255.0\n",
    "        img_tensor = img_tensor.to(Config.DEVICE)\n",
    "        with torch.no_grad():\n",
    "            output = model([img_tensor])[0]\n",
    "        pred_img = img_rgb.copy()\n",
    "        mask = (output['scores'] > Config.CONF_THRESHOLD) & (output['labels'] == 1)\n",
    "        boxes = output['boxes'][mask].cpu().numpy()\n",
    "        scores = output['scores'][mask].cpu().numpy()\n",
    "        for box, score in zip(boxes, scores):\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            cv2.rectangle(pred_img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "            cv2.putText(pred_img, f'{score:.2f}', (x1, y1-5),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "        axes[1, i].imshow(pred_img); axes[1, i].set_title(f'Pred ({len(boxes)})'); axes[1, i].axis('off')\n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "if Config.ENABLE_SYNTH_AUG and model_B is not None and ANNOTATIONS_PATH is not None:\n",
    "    print(\"Model B predictions on CLEAN test images:\")\n",
    "    fig1 = visualize_detections(model_B, IMAGES_DIR, TEST_ANN, \"Model B - Clean Test Set\")\n",
    "    fig1.savefig(f\"{Config.OUTPUT_DIR}/model_B_clean_predictions.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    if PERTURBED_TEST_ANN is not None and perturbed_dir is not None:\n",
    "        print(\"\\nModel B predictions on PERTURBED test images:\")\n",
    "        fig2 = visualize_detections(model_B, perturbed_dir, PERTURBED_TEST_ANN, \"Model B - Perturbed Test Set\")\n",
    "        fig2.savefig(f\"{Config.OUTPUT_DIR}/model_B_perturbed_predictions.png\", dpi=150)\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Synthetic augmentation disabled; skipping Model B visualization.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 16: Save Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# CELL 16: SAVE EXPERIMENT SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "summary = {\n",
    "    'config': {\n",
    "        'image_size': Config.IMG_SIZE,\n",
    "        'num_epochs': Config.NUM_EPOCHS,\n",
    "        'batch_size': Config.BATCH_SIZE,\n",
    "        'learning_rate': Config.LR,\n",
    "        'iou_threshold': Config.IOU_THRESHOLD,\n",
    "        'conf_threshold': Config.CONF_THRESHOLD,\n",
    "        'use_yolo_direct': Config.USE_YOLO_DIRECT,\n",
    "        'enable_synth_aug': Config.ENABLE_SYNTH_AUG\n",
    "    },\n",
    "    'model_A_baseline': {\n",
    "        'training_history': history_A,\n",
    "        'test_clean': metrics_A_clean,\n",
    "    }\n",
    "}\n",
    "\n",
    "if Config.ENABLE_SYNTH_AUG and model_B is not None:\n",
    "    summary['model_B_augmented'] = {\n",
    "        'training_history': history_B,\n",
    "        'best_f1': best_f1_B\n",
    "    }\n",
    "\n",
    "with open(f\"{Config.OUTPUT_DIR}/experiment_summary.json\", 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\"\"\n",
    "Saved outputs:\n",
    "  - Checkpoints: {Config.CHECKPOINT_DIR}/\n",
    "    - model_A_baseline_best.pth\n",
    "  - Visualizations: {Config.OUTPUT_DIR}/\n",
    "    - dataset_loading_test.png\n",
    "  - Summary: {Config.OUTPUT_DIR}/experiment_summary.json\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "### Key Fixes in This Version:\n",
    "1. **Box Format**: Proper conversion from COCO [x,y,w,h] to Faster R-CNN [x1,y1,x2,y2]\n",
    "2. **Evaluation**: Greedy matching sorted by confidence, proper FP/FN counting\n",
    "3. **Class Filtering**: Only count predictions with label=1 (person)\n",
    "4. **Debug Info**: TP/FP/FN printed each epoch for verification\n",
    "\n",
    "### Why Recall=1 Was Happening:\n",
    "- The original code may have had box format mismatches\n",
    "- Or the GT boxes were being compared incorrectly\n",
    "- This version properly handles all conversions\n",
    "\n",
    "### Swapping to YOLOv8:\n",
    "```python\n",
    "from ultralytics import YOLO\n",
    "model = YOLO('yolov8n.pt')\n",
    "model.train(data='data.yaml', epochs=6, imgsz=512, batch=4)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
