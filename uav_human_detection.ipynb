{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust Human Detection in UAV Imagery\n",
    "## HIT-UAV Infrared Thermal Dataset - Baseline vs Augmented Comparison\n",
    "\n",
    "**Experiment Design:**\n",
    "- **Model A**: Trained on clean/normal data only\n",
    "- **Model B**: Trained with SAR augmentations (snow, smoke/fire, thermal artifacts)\n",
    "- **Evaluation**: Compare both on clean and perturbed test sets\n",
    "\n",
    "**Dataset**: HIT-UAV from Kaggle (thermal infrared UAV imagery)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: ENVIRONMENT SETUP\n",
    "# =============================================================================\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Install required packages.\"\"\"\n",
    "    packages = [\n",
    "        'torch', 'torchvision', 'albumentations>=1.3.0', 'pycocotools',\n",
    "        'opencv-python-headless', 'matplotlib', 'numpy', 'Pillow',\n",
    "        'tqdm', 'scipy', 'kaggle'\n",
    "    ]\n",
    "    for pkg in packages:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pkg])\n",
    "    print(\"Packages installed\")\n",
    "\n",
    "install_packages()\n",
    "\n",
    "# Mount Google Drive (Colab) or use local cache\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DRIVE_ROOT = '/content/drive/MyDrive/uav_detection'\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    DRIVE_ROOT = './uav_detection_cache'\n",
    "    IN_COLAB = False\n",
    "\n",
    "os.makedirs(DRIVE_ROOT, exist_ok=True)\n",
    "os.makedirs(f\"{DRIVE_ROOT}/data\", exist_ok=True)\n",
    "os.makedirs(f\"{DRIVE_ROOT}/checkpoints\", exist_ok=True)\n",
    "os.makedirs(f\"{DRIVE_ROOT}/outputs\", exist_ok=True)\n",
    "\n",
    "print(f\"Cache directory: {DRIVE_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: IMPORTS AND CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    # Paths\n",
    "    DATA_ROOT = f\"{DRIVE_ROOT}/data/hit_uav\"\n",
    "    CURATED_ROOT = f\"{DRIVE_ROOT}/data/curated\"\n",
    "    CHECKPOINT_DIR = f\"{DRIVE_ROOT}/checkpoints\"\n",
    "    OUTPUT_DIR = f\"{DRIVE_ROOT}/outputs\"\n",
    "    \n",
    "    # Image settings\n",
    "    IMG_SIZE = 512\n",
    "    \n",
    "    # Training settings\n",
    "    BATCH_SIZE = 4\n",
    "    NUM_EPOCHS = 6\n",
    "    LR = 0.005\n",
    "    LR_STEP_SIZE = 3\n",
    "    LR_GAMMA = 0.1\n",
    "    WEIGHT_DECAY = 0.0005\n",
    "    \n",
    "    # Detection settings\n",
    "    NUM_CLASSES = 2  # background + person\n",
    "    IOU_THRESHOLD = 0.5\n",
    "    CONF_THRESHOLD = 0.5\n",
    "    \n",
    "    # Device\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    SEED = 42\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(Config.SEED)\n",
    "np.random.seed(Config.SEED)\n",
    "random.seed(Config.SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(Config.SEED)\n",
    "\n",
    "print(f\"Device: {Config.DEVICE}\")\n",
    "print(f\"Image size: {Config.IMG_SIZE}, Epochs: {Config.NUM_EPOCHS}, Batch: {Config.BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Download HIT-UAV Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: DOWNLOAD HIT-UAV DATASET FROM KAGGLE\n",
    "# =============================================================================\n",
    "\n",
    "def download_hituav_kaggle(data_root: str) -> bool:\n",
    "    \"\"\"\n",
    "    Download HIT-UAV dataset from Kaggle.\n",
    "    Requires Kaggle API credentials (~/.kaggle/kaggle.json or KAGGLE_USERNAME/KAGGLE_KEY env vars)\n",
    "    \"\"\"\n",
    "    data_root = Path(data_root)\n",
    "    data_root.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Check if already downloaded\n",
    "    images_dir = data_root / \"normal\" / \"images\"\n",
    "    if images_dir.exists() and len(list(images_dir.glob(\"*.jpg\"))) > 100:\n",
    "        print(f\"Dataset already exists at {data_root}\")\n",
    "        return True\n",
    "    \n",
    "    zip_path = data_root / \"hituav.zip\"\n",
    "    \n",
    "    # Method 1: Try curl download\n",
    "    print(\"Downloading HIT-UAV dataset from Kaggle...\")\n",
    "    try:\n",
    "        import subprocess\n",
    "        result = subprocess.run([\n",
    "            'curl', '-L', '-o', str(zip_path),\n",
    "            'https://www.kaggle.com/api/v1/datasets/download/pandrii000/hituav-a-highaltitude-infrared-thermal-dataset'\n",
    "        ], capture_output=True, timeout=600)\n",
    "        \n",
    "        if zip_path.exists() and zip_path.stat().st_size > 1000000:  # >1MB\n",
    "            print(f\"Downloaded to {zip_path}\")\n",
    "        else:\n",
    "            raise Exception(\"Download failed or file too small\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Curl download failed: {e}\")\n",
    "        \n",
    "        # Method 2: Try kaggle API\n",
    "        try:\n",
    "            import kaggle\n",
    "            kaggle.api.dataset_download_files(\n",
    "                'pandrii000/hituav-a-highaltitude-infrared-thermal-dataset',\n",
    "                path=str(data_root),\n",
    "                unzip=False\n",
    "            )\n",
    "            # Find the downloaded zip\n",
    "            for f in data_root.glob(\"*.zip\"):\n",
    "                zip_path = f\n",
    "                break\n",
    "        except Exception as e2:\n",
    "            print(f\"Kaggle API failed: {e2}\")\n",
    "            print(\"\\nPlease download manually:\")\n",
    "            print(\"1. Go to: https://www.kaggle.com/datasets/pandrii000/hituav-a-highaltitude-infrared-thermal-dataset\")\n",
    "            print(\"2. Download and extract to:\", data_root)\n",
    "            return False\n",
    "    \n",
    "    # Extract\n",
    "    if zip_path.exists():\n",
    "        print(\"Extracting dataset...\")\n",
    "        try:\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "                zf.extractall(data_root)\n",
    "            print(f\"Extracted to {data_root}\")\n",
    "            zip_path.unlink()  # Remove zip\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Extraction failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def explore_dataset_structure(data_root: str):\n",
    "    \"\"\"Print the dataset structure to understand it.\"\"\"\n",
    "    data_root = Path(data_root)\n",
    "    print(f\"\\nDataset structure at {data_root}:\")\n",
    "    \n",
    "    for item in sorted(data_root.rglob(\"*\")):\n",
    "        if item.is_dir():\n",
    "            files = list(item.glob(\"*\"))\n",
    "            depth = len(item.relative_to(data_root).parts)\n",
    "            indent = \"  \" * depth\n",
    "            print(f\"{indent}{item.name}/ ({len(files)} items)\")\n",
    "            # Show sample files\n",
    "            if len(files) > 0 and files[0].is_file():\n",
    "                print(f\"{indent}  Sample: {files[0].name}\")\n",
    "\n",
    "\n",
    "# Download dataset\n",
    "download_success = download_hituav_kaggle(Config.DATA_ROOT)\n",
    "\n",
    "if download_success:\n",
    "    explore_dataset_structure(Config.DATA_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Convert HIT-UAV to COCO Format (Person Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: CONVERT HIT-UAV TO COCO FORMAT\n",
    "# HIT-UAV uses YOLO format: class_id cx cy w h (normalized)\n",
    "# We convert to COCO: {images: [], annotations: [], categories: []}\n",
    "# =============================================================================\n",
    "\n",
    "def convert_hituav_to_coco(data_root: str, output_path: str, target_size: int = 512) -> Tuple[Path, Path]:\n",
    "    \"\"\"\n",
    "    Convert HIT-UAV YOLO format to COCO JSON format.\n",
    "    Only keeps 'Person' class (class_id=0 in HIT-UAV).\n",
    "    \n",
    "    HIT-UAV classes: 0=Person, 1=Car, 2=Bicycle, 3=OtherVehicle, 4=DontCare\n",
    "    \"\"\"\n",
    "    data_root = Path(data_root)\n",
    "    output_path = Path(output_path)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    images_out = output_path / \"images\"\n",
    "    images_out.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Find the images and labels directories\n",
    "    # HIT-UAV structure: normal/images/, normal/labels/, or train/images, etc.\n",
    "    possible_paths = [\n",
    "        (data_root / \"normal\" / \"images\", data_root / \"normal\" / \"labels\"),\n",
    "        (data_root / \"train\" / \"images\", data_root / \"train\" / \"labels\"),\n",
    "        (data_root / \"images\", data_root / \"labels\"),\n",
    "    ]\n",
    "    \n",
    "    images_dir = None\n",
    "    labels_dir = None\n",
    "    \n",
    "    for img_dir, lbl_dir in possible_paths:\n",
    "        if img_dir.exists():\n",
    "            images_dir = img_dir\n",
    "            labels_dir = lbl_dir\n",
    "            print(f\"Found images at: {images_dir}\")\n",
    "            print(f\"Found labels at: {labels_dir}\")\n",
    "            break\n",
    "    \n",
    "    if images_dir is None:\n",
    "        # List what we have\n",
    "        print(\"Could not find standard structure. Contents:\")\n",
    "        for item in data_root.iterdir():\n",
    "            print(f\"  {item}\")\n",
    "        raise FileNotFoundError(f\"Cannot find images directory in {data_root}\")\n",
    "    \n",
    "    # COCO format\n",
    "    coco = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": [{\"id\": 1, \"name\": \"person\", \"supercategory\": \"human\"}]\n",
    "    }\n",
    "    \n",
    "    PERSON_CLASS = 0  # HIT-UAV person class\n",
    "    ann_id = 1\n",
    "    img_id = 1\n",
    "    \n",
    "    # Get all image files\n",
    "    image_files = sorted(list(images_dir.glob(\"*.jpg\")) + list(images_dir.glob(\"*.png\")))\n",
    "    print(f\"\\nFound {len(image_files)} images\")\n",
    "    \n",
    "    images_with_persons = 0\n",
    "    total_persons = 0\n",
    "    \n",
    "    for img_path in tqdm(image_files, desc=\"Converting\"):\n",
    "        # Read image to get dimensions\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "            continue\n",
    "        \n",
    "        orig_h, orig_w = img.shape[:2]\n",
    "        \n",
    "        # Look for corresponding label file\n",
    "        label_path = labels_dir / f\"{img_path.stem}.txt\"\n",
    "        \n",
    "        person_annotations = []\n",
    "        \n",
    "        if label_path.exists():\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 5:\n",
    "                        cls_id = int(parts[0])\n",
    "                        if cls_id == PERSON_CLASS:\n",
    "                            # YOLO format: class cx cy w h (normalized 0-1)\n",
    "                            cx, cy, bw, bh = map(float, parts[1:5])\n",
    "                            \n",
    "                            # Convert to pixel coordinates\n",
    "                            x = (cx - bw / 2) * orig_w\n",
    "                            y = (cy - bh / 2) * orig_h\n",
    "                            w = bw * orig_w\n",
    "                            h = bh * orig_h\n",
    "                            \n",
    "                            # Clip to image bounds\n",
    "                            x = max(0, x)\n",
    "                            y = max(0, y)\n",
    "                            w = min(w, orig_w - x)\n",
    "                            h = min(h, orig_h - y)\n",
    "                            \n",
    "                            if w > 5 and h > 5:  # Skip tiny boxes\n",
    "                                person_annotations.append([x, y, w, h])\n",
    "        \n",
    "        # Only include images with person annotations\n",
    "        if len(person_annotations) == 0:\n",
    "            continue\n",
    "        \n",
    "        images_with_persons += 1\n",
    "        \n",
    "        # Resize image\n",
    "        scale_x = target_size / orig_w\n",
    "        scale_y = target_size / orig_h\n",
    "        img_resized = cv2.resize(img, (target_size, target_size))\n",
    "        \n",
    "        # Save resized image\n",
    "        new_filename = f\"hituav_{img_id:05d}.jpg\"\n",
    "        cv2.imwrite(str(images_out / new_filename), img_resized)\n",
    "        \n",
    "        # Add image info\n",
    "        coco[\"images\"].append({\n",
    "            \"id\": img_id,\n",
    "            \"file_name\": new_filename,\n",
    "            \"width\": target_size,\n",
    "            \"height\": target_size,\n",
    "            \"original_file\": img_path.name\n",
    "        })\n",
    "        \n",
    "        # Add scaled annotations\n",
    "        for (x, y, w, h) in person_annotations:\n",
    "            # Scale to new size\n",
    "            x_scaled = x * scale_x\n",
    "            y_scaled = y * scale_y\n",
    "            w_scaled = w * scale_x\n",
    "            h_scaled = h * scale_y\n",
    "            \n",
    "            # Skip if too small after scaling\n",
    "            if w_scaled < 8 or h_scaled < 8:\n",
    "                continue\n",
    "            \n",
    "            coco[\"annotations\"].append({\n",
    "                \"id\": ann_id,\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": 1,\n",
    "                \"bbox\": [x_scaled, y_scaled, w_scaled, h_scaled],  # COCO format: x, y, w, h\n",
    "                \"area\": w_scaled * h_scaled,\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "            ann_id += 1\n",
    "            total_persons += 1\n",
    "        \n",
    "        img_id += 1\n",
    "    \n",
    "    # Save annotations\n",
    "    ann_path = output_path / \"annotations.json\"\n",
    "    with open(ann_path, 'w') as f:\n",
    "        json.dump(coco, f)\n",
    "    \n",
    "    print(f\"\\nConversion complete:\")\n",
    "    print(f\"  Images with persons: {images_with_persons}\")\n",
    "    print(f\"  Total person annotations: {total_persons}\")\n",
    "    print(f\"  Avg persons per image: {total_persons/max(images_with_persons,1):.1f}\")\n",
    "    print(f\"  Saved to: {output_path}\")\n",
    "    \n",
    "    return images_out, ann_path\n",
    "\n",
    "\n",
    "# Convert dataset\n",
    "print(\"Converting HIT-UAV to COCO format...\")\n",
    "IMAGES_DIR, ANNOTATIONS_PATH = convert_hituav_to_coco(\n",
    "    Config.DATA_ROOT,\n",
    "    Config.CURATED_ROOT,\n",
    "    target_size=Config.IMG_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: SAR Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: SAR AUGMENTATIONS\n",
    "# Snow, Smoke/Fire, Thermal artifacts for robustness\n",
    "# =============================================================================\n",
    "\n",
    "class SARaugmentations:\n",
    "    \"\"\"Realistic augmentations for SAR drone imagery.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_perlin_noise(shape: Tuple[int, int], scale: float = 100.0) -> np.ndarray:\n",
    "        \"\"\"Generate Perlin-like noise using octaves of Gaussian noise.\"\"\"\n",
    "        h, w = shape\n",
    "        noise = np.zeros((h, w), dtype=np.float32)\n",
    "        \n",
    "        for octave in range(4):\n",
    "            freq = 2 ** octave\n",
    "            amplitude = 1.0 / freq\n",
    "            small_h = max(2, int(h / (scale / freq)))\n",
    "            small_w = max(2, int(w / (scale / freq)))\n",
    "            small_noise = np.random.randn(small_h, small_w).astype(np.float32)\n",
    "            upscaled = cv2.resize(small_noise, (w, h), interpolation=cv2.INTER_CUBIC)\n",
    "            noise += amplitude * upscaled\n",
    "        \n",
    "        noise = (noise - noise.min()) / (noise.max() - noise.min() + 1e-8)\n",
    "        return noise\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_snow(img: np.ndarray, intensity: float = 0.5) -> np.ndarray:\n",
    "        \"\"\"Apply realistic snow effect.\"\"\"\n",
    "        h, w = img.shape[:2]\n",
    "        is_color = len(img.shape) == 3\n",
    "        \n",
    "        snow_noise = SARaugmentations.generate_perlin_noise((h, w), scale=50.0)\n",
    "        fine_noise = np.random.rand(h, w).astype(np.float32)\n",
    "        fine_noise = cv2.GaussianBlur(fine_noise, (5, 5), 0)\n",
    "        \n",
    "        snow_layer = 0.6 * snow_noise + 0.4 * fine_noise\n",
    "        snow_layer = np.clip(snow_layer * intensity * 255, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        if is_color:\n",
    "            snow_layer = cv2.cvtColor(snow_layer, cv2.COLOR_GRAY2BGR)\n",
    "        \n",
    "        img_float = img.astype(np.float32)\n",
    "        mean_val = np.mean(img_float)\n",
    "        contrast_reduction = 0.3\n",
    "        img_reduced = (1 - contrast_reduction) * img_float + contrast_reduction * mean_val\n",
    "        \n",
    "        alpha = intensity * 0.7\n",
    "        result = (1 - alpha) * img_reduced + alpha * snow_layer.astype(np.float32)\n",
    "        return np.clip(result, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_smoke_fire(img: np.ndarray, smoke_intensity: float = 0.4, \n",
    "                         fire_intensity: float = 0.3) -> np.ndarray:\n",
    "        \"\"\"Apply smoke and fire effects.\"\"\"\n",
    "        h, w = img.shape[:2]\n",
    "        if len(img.shape) != 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "        \n",
    "        result = img.astype(np.float32)\n",
    "        \n",
    "        # Smoke\n",
    "        smoke_noise = SARaugmentations.generate_perlin_noise((h, w), scale=80.0)\n",
    "        gradient = np.linspace(1.0, 0.3, h).reshape(-1, 1)\n",
    "        gradient = np.tile(gradient, (1, w))\n",
    "        smoke_mask = smoke_noise * gradient\n",
    "        \n",
    "        smoke_color = np.array([180, 180, 180], dtype=np.float32)\n",
    "        smoke_layer = np.ones((h, w, 3), dtype=np.float32) * smoke_color\n",
    "        smoke_layer = cv2.GaussianBlur(smoke_layer, (21, 21), 0)\n",
    "        \n",
    "        smoke_alpha = smoke_mask[..., np.newaxis] * smoke_intensity\n",
    "        result = result * (1 - smoke_alpha) + smoke_layer * smoke_alpha\n",
    "        \n",
    "        # Fire\n",
    "        if fire_intensity > 0:\n",
    "            fx, fy = np.random.randint(w//4, 3*w//4), np.random.randint(h//2, h)\n",
    "            y_coords, x_coords = np.ogrid[:h, :w]\n",
    "            dist = np.sqrt((x_coords - fx)**2 + (y_coords - fy)**2)\n",
    "            fire_radius = min(h, w) // 3\n",
    "            fire_mask = np.clip(1 - dist / fire_radius, 0, 1) ** 2\n",
    "            \n",
    "            fire_color = np.array([30, 100, 255], dtype=np.float32)  # Orange BGR\n",
    "            fire_layer = np.ones((h, w, 3), dtype=np.float32) * fire_color\n",
    "            \n",
    "            fire_alpha = fire_mask[..., np.newaxis] * fire_intensity\n",
    "            result = result * (1 - fire_alpha) + fire_layer * fire_alpha\n",
    "        \n",
    "        return np.clip(result, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_thermal_artifacts(img: np.ndarray, intensity_scale: float = 1.0,\n",
    "                                sensor_noise: float = 0.05) -> np.ndarray:\n",
    "        \"\"\"Apply thermal camera artifacts.\"\"\"\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        if len(img.shape) == 3:\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray = img.copy()\n",
    "        \n",
    "        result = gray.astype(np.float32) * intensity_scale\n",
    "        \n",
    "        # Sensor noise\n",
    "        noise = np.random.normal(0, sensor_noise * 255, (h, w)).astype(np.float32)\n",
    "        if np.random.rand() < 0.3:\n",
    "            line_noise = np.random.normal(0, sensor_noise * 50, (h, 1))\n",
    "            noise += np.tile(line_noise, (1, w))\n",
    "        \n",
    "        result += noise\n",
    "        result = np.clip(result, 0, 255).astype(np.uint8)\n",
    "        return cv2.cvtColor(result, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_random(img: np.ndarray) -> Tuple[np.ndarray, str]:\n",
    "        \"\"\"Apply random SAR augmentation.\"\"\"\n",
    "        aug_type = np.random.choice(['snow', 'fire', 'thermal', 'none'])\n",
    "        \n",
    "        if aug_type == 'snow':\n",
    "            return SARaugmentations.apply_snow(img, np.random.uniform(0.3, 0.6)), 'snow'\n",
    "        elif aug_type == 'fire':\n",
    "            return SARaugmentations.apply_smoke_fire(img, np.random.uniform(0.2, 0.4),\n",
    "                                                     np.random.uniform(0.2, 0.4)), 'fire'\n",
    "        elif aug_type == 'thermal':\n",
    "            return SARaugmentations.apply_thermal_artifacts(img, np.random.uniform(0.8, 1.2),\n",
    "                                                            np.random.uniform(0.03, 0.08)), 'thermal'\n",
    "        return img, 'none'\n",
    "\n",
    "\n",
    "# Visualize augmentations\n",
    "sample_imgs = list(IMAGES_DIR.glob(\"*.jpg\"))[:1]\n",
    "if sample_imgs:\n",
    "    sample = cv2.imread(str(sample_imgs[0]))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    axes[0,0].imshow(cv2.cvtColor(sample, cv2.COLOR_BGR2RGB))\n",
    "    axes[0,0].set_title('Original')\n",
    "    axes[0,0].axis('off')\n",
    "    \n",
    "    axes[0,1].imshow(cv2.cvtColor(SARaugmentations.apply_snow(sample, 0.5), cv2.COLOR_BGR2RGB))\n",
    "    axes[0,1].set_title('Snow')\n",
    "    axes[0,1].axis('off')\n",
    "    \n",
    "    axes[1,0].imshow(cv2.cvtColor(SARaugmentations.apply_smoke_fire(sample, 0.4, 0.4), cv2.COLOR_BGR2RGB))\n",
    "    axes[1,0].set_title('Smoke/Fire')\n",
    "    axes[1,0].axis('off')\n",
    "    \n",
    "    axes[1,1].imshow(cv2.cvtColor(SARaugmentations.apply_thermal_artifacts(sample, 1.1, 0.06), cv2.COLOR_BGR2RGB))\n",
    "    axes[1,1].set_title('Thermal Artifacts')\n",
    "    axes[1,1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{Config.OUTPUT_DIR}/augmentations.png\", dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Dataset Class with Proper Box Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: DATASET CLASS\n",
    "# Proper handling of box formats to avoid evaluation bugs\n",
    "# =============================================================================\n",
    "\n",
    "class UAVDetectionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for UAV person detection with proper box format handling.\n",
    "    \n",
    "    IMPORTANT: Boxes in COCO are [x, y, width, height]\n",
    "    Faster R-CNN expects [x1, y1, x2, y2] (pascal_voc format)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, images_dir, annotations_path, transforms=None,\n",
    "                 apply_sar_aug=False, sar_aug_prob=0.5):\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.transforms = transforms\n",
    "        self.apply_sar_aug = apply_sar_aug\n",
    "        self.sar_aug_prob = sar_aug_prob\n",
    "        \n",
    "        with open(annotations_path, 'r') as f:\n",
    "            coco = json.load(f)\n",
    "        \n",
    "        self.images = {img['id']: img for img in coco['images']}\n",
    "        \n",
    "        # Group annotations by image\n",
    "        self.img_to_anns = defaultdict(list)\n",
    "        for ann in coco['annotations']:\n",
    "            self.img_to_anns[ann['image_id']].append(ann)\n",
    "        \n",
    "        # Only keep images WITH annotations\n",
    "        self.img_ids = [img_id for img_id in self.images.keys()\n",
    "                        if len(self.img_to_anns[img_id]) > 0]\n",
    "        \n",
    "        print(f\"Dataset: {len(self.img_ids)} images with annotations\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_info = self.images[img_id]\n",
    "        \n",
    "        # Load image\n",
    "        img_path = self.images_dir / img_info['file_name']\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Apply SAR augmentation\n",
    "        if self.apply_sar_aug and random.random() < self.sar_aug_prob:\n",
    "            img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "            img_aug, _ = SARaugmentations.apply_random(img_bgr)\n",
    "            img = cv2.cvtColor(img_aug, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Get annotations and convert boxes\n",
    "        anns = self.img_to_anns[img_id]\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        areas = []\n",
    "        \n",
    "        for ann in anns:\n",
    "            # COCO format: [x, y, width, height]\n",
    "            x, y, w, h = ann['bbox']\n",
    "            \n",
    "            # Convert to pascal_voc: [x1, y1, x2, y2]\n",
    "            x1 = x\n",
    "            y1 = y\n",
    "            x2 = x + w\n",
    "            y2 = y + h\n",
    "            \n",
    "            # Validate box\n",
    "            if x2 > x1 and y2 > y1:\n",
    "                boxes.append([x1, y1, x2, y2])\n",
    "                labels.append(1)  # Person class = 1\n",
    "                areas.append(ann['area'])\n",
    "        \n",
    "        # Convert image to tensor\n",
    "        img_tensor = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
    "        \n",
    "        # Apply additional transforms (normalization, etc.)\n",
    "        if self.transforms:\n",
    "            img_tensor = self.transforms(img_tensor)\n",
    "        \n",
    "        # Create target dict\n",
    "        if len(boxes) > 0:\n",
    "            boxes_tensor = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels_tensor = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            areas_tensor = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        else:\n",
    "            boxes_tensor = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels_tensor = torch.zeros((0,), dtype=torch.int64)\n",
    "            areas_tensor = torch.zeros((0,), dtype=torch.float32)\n",
    "        \n",
    "        target = {\n",
    "            'boxes': boxes_tensor,\n",
    "            'labels': labels_tensor,\n",
    "            'image_id': torch.tensor([img_id]),\n",
    "            'area': areas_tensor,\n",
    "            'iscrowd': torch.zeros(len(boxes), dtype=torch.int64)\n",
    "        }\n",
    "        \n",
    "        return img_tensor, target\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate for detection.\"\"\"\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "# Create train/val/test split (70/15/15)\n",
    "with open(ANNOTATIONS_PATH, 'r') as f:\n",
    "    full_coco = json.load(f)\n",
    "\n",
    "all_images = full_coco['images'].copy()\n",
    "random.shuffle(all_images)\n",
    "\n",
    "n = len(all_images)\n",
    "train_end = int(0.7 * n)\n",
    "val_end = int(0.85 * n)\n",
    "\n",
    "train_images = all_images[:train_end]\n",
    "val_images = all_images[train_end:val_end]\n",
    "test_images = all_images[val_end:]\n",
    "\n",
    "# Create ID sets\n",
    "train_ids = set(img['id'] for img in train_images)\n",
    "val_ids = set(img['id'] for img in val_images)\n",
    "test_ids = set(img['id'] for img in test_images)\n",
    "\n",
    "# Split annotations\n",
    "train_anns = [a for a in full_coco['annotations'] if a['image_id'] in train_ids]\n",
    "val_anns = [a for a in full_coco['annotations'] if a['image_id'] in val_ids]\n",
    "test_anns = [a for a in full_coco['annotations'] if a['image_id'] in test_ids]\n",
    "\n",
    "# Save splits\n",
    "splits = {\n",
    "    'train': (train_images, train_anns),\n",
    "    'val': (val_images, val_anns),\n",
    "    'test': (test_images, test_anns)\n",
    "}\n",
    "\n",
    "for split_name, (images, anns) in splits.items():\n",
    "    split_coco = {\n",
    "        'images': images,\n",
    "        'annotations': anns,\n",
    "        'categories': full_coco['categories']\n",
    "    }\n",
    "    with open(Path(Config.CURATED_ROOT) / f\"{split_name}.json\", 'w') as f:\n",
    "        json.dump(split_coco, f)\n",
    "    print(f\"{split_name}: {len(images)} images, {len(anns)} annotations\")\n",
    "\n",
    "TRAIN_ANN = Path(Config.CURATED_ROOT) / \"train.json\"\n",
    "VAL_ANN = Path(Config.CURATED_ROOT) / \"val.json\"\n",
    "TEST_ANN = Path(Config.CURATED_ROOT) / \"test.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Corrected Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 7: CORRECTED EVALUATION FUNCTION\n",
    "# Properly computes precision, recall, F1 with detailed debugging\n",
    "# =============================================================================\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Compute IoU between two boxes in [x1, y1, x2, y2] format.\n",
    "    \"\"\"\n",
    "    # Intersection\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    inter_w = max(0, x2 - x1)\n",
    "    inter_h = max(0, y2 - y1)\n",
    "    inter_area = inter_w * inter_h\n",
    "    \n",
    "    # Union\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union_area = area1 + area2 - inter_area\n",
    "    \n",
    "    if union_area <= 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return inter_area / union_area\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, data_loader, device, iou_thresh=0.5, conf_thresh=0.5, verbose=False):\n",
    "    \"\"\"\n",
    "    Evaluate detection model with proper metric computation.\n",
    "    \n",
    "    Returns precision, recall, F1 at given IoU and confidence thresholds.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    total_tp = 0\n",
    "    total_fp = 0\n",
    "    total_fn = 0\n",
    "    total_gt = 0\n",
    "    total_pred = 0\n",
    "    \n",
    "    for batch_idx, (images, targets) in enumerate(tqdm(data_loader, desc=\"Evaluating\", disable=not verbose)):\n",
    "        images = [img.to(device) for img in images]\n",
    "        \n",
    "        # Get predictions\n",
    "        outputs = model(images)\n",
    "        \n",
    "        for output, target in zip(outputs, targets):\n",
    "            # Get ground truth boxes (already in [x1, y1, x2, y2] format from dataset)\n",
    "            gt_boxes = target['boxes'].cpu().numpy()\n",
    "            total_gt += len(gt_boxes)\n",
    "            \n",
    "            # Filter predictions by confidence AND class (person = 1)\n",
    "            scores = output['scores'].cpu().numpy()\n",
    "            pred_boxes = output['boxes'].cpu().numpy()\n",
    "            pred_labels = output['labels'].cpu().numpy()\n",
    "            \n",
    "            # Only keep person predictions above threshold\n",
    "            mask = (scores >= conf_thresh) & (pred_labels == 1)\n",
    "            pred_boxes = pred_boxes[mask]\n",
    "            pred_scores = scores[mask]\n",
    "            total_pred += len(pred_boxes)\n",
    "            \n",
    "            if len(gt_boxes) == 0:\n",
    "                # All predictions are false positives\n",
    "                total_fp += len(pred_boxes)\n",
    "                continue\n",
    "            \n",
    "            if len(pred_boxes) == 0:\n",
    "                # All ground truths are missed\n",
    "                total_fn += len(gt_boxes)\n",
    "                continue\n",
    "            \n",
    "            # Match predictions to ground truth (greedy matching by score)\n",
    "            # Sort predictions by score (descending)\n",
    "            sorted_indices = np.argsort(-pred_scores)\n",
    "            matched_gt = set()\n",
    "            \n",
    "            for pred_idx in sorted_indices:\n",
    "                pred_box = pred_boxes[pred_idx]\n",
    "                \n",
    "                best_iou = 0\n",
    "                best_gt_idx = -1\n",
    "                \n",
    "                for gt_idx, gt_box in enumerate(gt_boxes):\n",
    "                    if gt_idx in matched_gt:\n",
    "                        continue\n",
    "                    \n",
    "                    iou = compute_iou(pred_box, gt_box)\n",
    "                    if iou > best_iou:\n",
    "                        best_iou = iou\n",
    "                        best_gt_idx = gt_idx\n",
    "                \n",
    "                if best_iou >= iou_thresh and best_gt_idx >= 0:\n",
    "                    total_tp += 1\n",
    "                    matched_gt.add(best_gt_idx)\n",
    "                else:\n",
    "                    total_fp += 1\n",
    "            \n",
    "            # Unmatched ground truths are false negatives\n",
    "            total_fn += len(gt_boxes) - len(matched_gt)\n",
    "    \n",
    "    # Compute metrics\n",
    "    precision = total_tp / max(total_tp + total_fp, 1)\n",
    "    recall = total_tp / max(total_tp + total_fn, 1)\n",
    "    f1 = 2 * precision * recall / max(precision + recall, 1e-8)\n",
    "    \n",
    "    metrics = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'tp': total_tp,\n",
    "        'fp': total_fp,\n",
    "        'fn': total_fn,\n",
    "        'total_gt': total_gt,\n",
    "        'total_pred': total_pred\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nEvaluation Results (IoU={iou_thresh}, Conf={conf_thresh}):\")\n",
    "        print(f\"  GT boxes: {total_gt}, Predictions: {total_pred}\")\n",
    "        print(f\"  TP: {total_tp}, FP: {total_fp}, FN: {total_fn}\")\n",
    "        print(f\"  Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "print(\"Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Model Creation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: MODEL CREATION\n",
    "# =============================================================================\n",
    "\n",
    "def create_detection_model(num_classes=2, pretrained=True, freeze_backbone=True):\n",
    "    \"\"\"\n",
    "    Create Faster R-CNN model for person detection.\n",
    "    \n",
    "    Args:\n",
    "        num_classes: 2 (background + person)\n",
    "        pretrained: Use COCO pretrained weights\n",
    "        freeze_backbone: Freeze early layers to prevent overfitting\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "        model = fasterrcnn_resnet50_fpn(weights=weights)\n",
    "    else:\n",
    "        model = fasterrcnn_resnet50_fpn(weights=None)\n",
    "    \n",
    "    # Replace the classifier head\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    if freeze_backbone:\n",
    "        # Freeze backbone except layer4 and FPN\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'backbone' in name:\n",
    "                if 'layer4' not in name and 'fpn' not in name:\n",
    "                    param.requires_grad = False\n",
    "    \n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model: {trainable:,} / {total:,} trainable params ({100*trainable/total:.1f}%)\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# YOLOV8 ALTERNATIVE:\n",
    "# from ultralytics import YOLO\n",
    "# model = YOLO('yolov8n.pt')\n",
    "# model.train(data='data.yaml', epochs=6, imgsz=512)\n",
    "# ============================================\n",
    "\n",
    "print(\"Model creation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 9: TRAINING FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs, lr,\n",
    "                checkpoint_prefix=\"model\", lr_step=3, lr_gamma=0.1):\n",
    "    \"\"\"\n",
    "    Train the detection model and track metrics.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    \n",
    "    # Optimizer - only train unfrozen params\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=lr, momentum=0.9, weight_decay=Config.WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_step, gamma=lr_gamma)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_precision': [],\n",
    "        'val_recall': [],\n",
    "        'val_f1': []\n",
    "    }\n",
    "    \n",
    "    best_f1 = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for images, targets in pbar:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Skip batches with no valid targets\n",
    "            valid_targets = [t for t in targets if len(t['boxes']) > 0]\n",
    "            if len(valid_targets) == 0:\n",
    "                continue\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            losses.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(params, max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += losses.item()\n",
    "            num_batches += 1\n",
    "            pbar.set_postfix({'loss': f\"{losses.item():.4f}\"})\n",
    "        \n",
    "        avg_loss = epoch_loss / max(num_batches, 1)\n",
    "        history['train_loss'].append(avg_loss)\n",
    "        \n",
    "        # Validation\n",
    "        metrics = evaluate_model(model, val_loader, device,\n",
    "                                 iou_thresh=Config.IOU_THRESHOLD,\n",
    "                                 conf_thresh=Config.CONF_THRESHOLD)\n",
    "        \n",
    "        history['val_precision'].append(metrics['precision'])\n",
    "        history['val_recall'].append(metrics['recall'])\n",
    "        history['val_f1'].append(metrics['f1'])\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, \"\n",
    "              f\"P={metrics['precision']:.4f}, R={metrics['recall']:.4f}, F1={metrics['f1']:.4f} \"\n",
    "              f\"(TP={metrics['tp']}, FP={metrics['fp']}, FN={metrics['fn']})\")\n",
    "        \n",
    "        # Save best model\n",
    "        if metrics['f1'] > best_f1:\n",
    "            best_f1 = metrics['f1']\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'metrics': metrics,\n",
    "                'history': history\n",
    "            }, f\"{Config.CHECKPOINT_DIR}/{checkpoint_prefix}_best.pth\")\n",
    "            print(f\"  -> Saved best model (F1={best_f1:.4f})\")\n",
    "    \n",
    "    return model, history, best_f1\n",
    "\n",
    "\n",
    "print(\"Training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Train Model A (Baseline - No SAR Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 10: TRAIN MODEL A - BASELINE (NO AUGMENTATION)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING MODEL A: BASELINE (No SAR Augmentation)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create datasets WITHOUT SAR augmentation\n",
    "train_dataset_baseline = UAVDetectionDataset(\n",
    "    IMAGES_DIR, TRAIN_ANN,\n",
    "    apply_sar_aug=False\n",
    ")\n",
    "\n",
    "val_dataset = UAVDetectionDataset(\n",
    "    IMAGES_DIR, VAL_ANN,\n",
    "    apply_sar_aug=False\n",
    ")\n",
    "\n",
    "train_loader_baseline = DataLoader(\n",
    "    train_dataset_baseline,\n",
    "    batch_size=Config.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=Config.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Create and train model A\n",
    "model_A = create_detection_model(\n",
    "    num_classes=Config.NUM_CLASSES,\n",
    "    pretrained=True,\n",
    "    freeze_backbone=True\n",
    ")\n",
    "\n",
    "model_A, history_A, best_f1_A = train_model(\n",
    "    model_A, train_loader_baseline, val_loader, Config.DEVICE,\n",
    "    num_epochs=Config.NUM_EPOCHS,\n",
    "    lr=Config.LR,\n",
    "    checkpoint_prefix=\"model_A_baseline\",\n",
    "    lr_step=Config.LR_STEP_SIZE,\n",
    "    lr_gamma=Config.LR_GAMMA\n",
    ")\n",
    "\n",
    "print(f\"\\nModel A Best F1: {best_f1_A:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Train Model B (With SAR Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 11: TRAIN MODEL B - WITH SAR AUGMENTATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING MODEL B: WITH SAR AUGMENTATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create dataset WITH SAR augmentation\n",
    "train_dataset_augmented = UAVDetectionDataset(\n",
    "    IMAGES_DIR, TRAIN_ANN,\n",
    "    apply_sar_aug=True,\n",
    "    sar_aug_prob=0.5  # 50% chance of augmentation\n",
    ")\n",
    "\n",
    "train_loader_augmented = DataLoader(\n",
    "    train_dataset_augmented,\n",
    "    batch_size=Config.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Create and train model B (fresh model)\n",
    "model_B = create_detection_model(\n",
    "    num_classes=Config.NUM_CLASSES,\n",
    "    pretrained=True,\n",
    "    freeze_backbone=True\n",
    ")\n",
    "\n",
    "model_B, history_B, best_f1_B = train_model(\n",
    "    model_B, train_loader_augmented, val_loader, Config.DEVICE,\n",
    "    num_epochs=Config.NUM_EPOCHS,\n",
    "    lr=Config.LR,\n",
    "    checkpoint_prefix=\"model_B_augmented\",\n",
    "    lr_step=Config.LR_STEP_SIZE,\n",
    "    lr_gamma=Config.LR_GAMMA\n",
    ")\n",
    "\n",
    "print(f\"\\nModel B Best F1: {best_f1_B:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12: Create Perturbed Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 12: CREATE PERTURBED TEST SET\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Creating perturbed test set...\")\n",
    "\n",
    "perturbed_dir = Path(Config.CURATED_ROOT) / \"perturbed_test\"\n",
    "perturbed_dir.mkdir(exist_ok=True)\n",
    "\n",
    "with open(TEST_ANN, 'r') as f:\n",
    "    test_coco = json.load(f)\n",
    "\n",
    "for img_info in tqdm(test_coco['images'], desc=\"Perturbing test images\"):\n",
    "    img_path = IMAGES_DIR / img_info['file_name']\n",
    "    img = cv2.imread(str(img_path))\n",
    "    \n",
    "    if img is None:\n",
    "        continue\n",
    "    \n",
    "    # Apply random perturbation (snow or fire)\n",
    "    aug_type = random.choice(['snow', 'fire'])\n",
    "    if aug_type == 'snow':\n",
    "        perturbed = SARaugmentations.apply_snow(img, random.uniform(0.4, 0.6))\n",
    "    else:\n",
    "        perturbed = SARaugmentations.apply_smoke_fire(img, random.uniform(0.3, 0.5),\n",
    "                                                       random.uniform(0.3, 0.5))\n",
    "    \n",
    "    cv2.imwrite(str(perturbed_dir / img_info['file_name']), perturbed)\n",
    "\n",
    "# Save annotations (same as test)\n",
    "PERTURBED_TEST_ANN = perturbed_dir / \"annotations.json\"\n",
    "with open(PERTURBED_TEST_ANN, 'w') as f:\n",
    "    json.dump(test_coco, f)\n",
    "\n",
    "print(f\"Perturbed test set saved to {perturbed_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 13: Final Comparison - Both Models on Clean & Perturbed Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 13: FINAL COMPARISON\n",
    "# Evaluate both models on clean and perturbed test sets\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL COMPARISON: MODEL A (Baseline) vs MODEL B (Augmented)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load best checkpoints\n",
    "ckpt_A = torch.load(f\"{Config.CHECKPOINT_DIR}/model_A_baseline_best.pth\", map_location=Config.DEVICE)\n",
    "model_A.load_state_dict(ckpt_A['model_state_dict'])\n",
    "\n",
    "ckpt_B = torch.load(f\"{Config.CHECKPOINT_DIR}/model_B_augmented_best.pth\", map_location=Config.DEVICE)\n",
    "model_B.load_state_dict(ckpt_B['model_state_dict'])\n",
    "\n",
    "# Create test datasets\n",
    "test_dataset_clean = UAVDetectionDataset(IMAGES_DIR, TEST_ANN, apply_sar_aug=False)\n",
    "test_dataset_perturbed = UAVDetectionDataset(perturbed_dir, PERTURBED_TEST_ANN, apply_sar_aug=False)\n",
    "\n",
    "test_loader_clean = DataLoader(test_dataset_clean, batch_size=Config.BATCH_SIZE,\n",
    "                               shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
    "test_loader_perturbed = DataLoader(test_dataset_perturbed, batch_size=Config.BATCH_SIZE,\n",
    "                                   shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "# Evaluate Model A\n",
    "print(\"\\n--- Model A (Baseline) ---\")\n",
    "print(\"On CLEAN test set:\")\n",
    "metrics_A_clean = evaluate_model(model_A, test_loader_clean, Config.DEVICE,\n",
    "                                  iou_thresh=Config.IOU_THRESHOLD,\n",
    "                                  conf_thresh=Config.CONF_THRESHOLD, verbose=True)\n",
    "\n",
    "print(\"\\nOn PERTURBED test set:\")\n",
    "metrics_A_perturbed = evaluate_model(model_A, test_loader_perturbed, Config.DEVICE,\n",
    "                                      iou_thresh=Config.IOU_THRESHOLD,\n",
    "                                      conf_thresh=Config.CONF_THRESHOLD, verbose=True)\n",
    "\n",
    "# Evaluate Model B\n",
    "print(\"\\n--- Model B (Augmented) ---\")\n",
    "print(\"On CLEAN test set:\")\n",
    "metrics_B_clean = evaluate_model(model_B, test_loader_clean, Config.DEVICE,\n",
    "                                  iou_thresh=Config.IOU_THRESHOLD,\n",
    "                                  conf_thresh=Config.CONF_THRESHOLD, verbose=True)\n",
    "\n",
    "print(\"\\nOn PERTURBED test set:\")\n",
    "metrics_B_perturbed = evaluate_model(model_B, test_loader_perturbed, Config.DEVICE,\n",
    "                                      iou_thresh=Config.IOU_THRESHOLD,\n",
    "                                      conf_thresh=Config.CONF_THRESHOLD, verbose=True)\n",
    "\n",
    "# Summary Table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY TABLE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Model':<20} {'Test Set':<15} {'Precision':<12} {'Recall':<12} {'F1':<12}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'A (Baseline)':<20} {'Clean':<15} {metrics_A_clean['precision']:.4f}       {metrics_A_clean['recall']:.4f}       {metrics_A_clean['f1']:.4f}\")\n",
    "print(f\"{'A (Baseline)':<20} {'Perturbed':<15} {metrics_A_perturbed['precision']:.4f}       {metrics_A_perturbed['recall']:.4f}       {metrics_A_perturbed['f1']:.4f}\")\n",
    "print(f\"{'B (Augmented)':<20} {'Clean':<15} {metrics_B_clean['precision']:.4f}       {metrics_B_clean['recall']:.4f}       {metrics_B_clean['f1']:.4f}\")\n",
    "print(f\"{'B (Augmented)':<20} {'Perturbed':<15} {metrics_B_perturbed['precision']:.4f}       {metrics_B_perturbed['recall']:.4f}       {metrics_B_perturbed['f1']:.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Robustness analysis\n",
    "drop_A = (metrics_A_clean['f1'] - metrics_A_perturbed['f1']) / max(metrics_A_clean['f1'], 1e-8) * 100\n",
    "drop_B = (metrics_B_clean['f1'] - metrics_B_perturbed['f1']) / max(metrics_B_clean['f1'], 1e-8) * 100\n",
    "\n",
    "print(f\"\\nRobustness Analysis:\")\n",
    "print(f\"  Model A F1 drop on perturbed: {drop_A:.1f}%\")\n",
    "print(f\"  Model B F1 drop on perturbed: {drop_B:.1f}%\")\n",
    "print(f\"  Robustness improvement: {drop_A - drop_B:.1f}%\")\n",
    "\n",
    "if drop_B < drop_A:\n",
    "    print(\"\\n-> Model B (with SAR augmentation) is MORE ROBUST to adverse conditions!\")\n",
    "else:\n",
    "    print(\"\\n-> Augmentation did not improve robustness (may need tuning)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 14: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 14: VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "# Training curves comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Training loss\n",
    "axes[0, 0].plot(history_A['train_loss'], 'b-o', label='Model A (Baseline)')\n",
    "axes[0, 0].plot(history_B['train_loss'], 'r-s', label='Model B (Augmented)')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# F1 Score\n",
    "axes[0, 1].plot(history_A['val_f1'], 'b-o', label='Model A (Baseline)')\n",
    "axes[0, 1].plot(history_B['val_f1'], 'r-s', label='Model B (Augmented)')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('F1 Score')\n",
    "axes[0, 1].set_title('Validation F1')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Precision\n",
    "axes[1, 0].plot(history_A['val_precision'], 'b-o', label='Model A (Baseline)')\n",
    "axes[1, 0].plot(history_B['val_precision'], 'r-s', label='Model B (Augmented)')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Precision')\n",
    "axes[1, 0].set_title('Validation Precision')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Recall\n",
    "axes[1, 1].plot(history_A['val_recall'], 'b-o', label='Model A (Baseline)')\n",
    "axes[1, 1].plot(history_B['val_recall'], 'r-s', label='Model B (Augmented)')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Recall')\n",
    "axes[1, 1].set_title('Validation Recall')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{Config.OUTPUT_DIR}/training_comparison.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Final metrics bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(3)\n",
    "width = 0.2\n",
    "\n",
    "metrics_labels = ['Precision', 'Recall', 'F1']\n",
    "a_clean = [metrics_A_clean['precision'], metrics_A_clean['recall'], metrics_A_clean['f1']]\n",
    "a_perturbed = [metrics_A_perturbed['precision'], metrics_A_perturbed['recall'], metrics_A_perturbed['f1']]\n",
    "b_clean = [metrics_B_clean['precision'], metrics_B_clean['recall'], metrics_B_clean['f1']]\n",
    "b_perturbed = [metrics_B_perturbed['precision'], metrics_B_perturbed['recall'], metrics_B_perturbed['f1']]\n",
    "\n",
    "bars1 = ax.bar(x - 1.5*width, a_clean, width, label='A-Clean', color='blue', alpha=0.8)\n",
    "bars2 = ax.bar(x - 0.5*width, a_perturbed, width, label='A-Perturbed', color='blue', alpha=0.4)\n",
    "bars3 = ax.bar(x + 0.5*width, b_clean, width, label='B-Clean', color='red', alpha=0.8)\n",
    "bars4 = ax.bar(x + 1.5*width, b_perturbed, width, label='B-Perturbed', color='red', alpha=0.4)\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Comparison: Clean vs Perturbed Test Sets')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics_labels)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{Config.OUTPUT_DIR}/metrics_comparison.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVisualizations saved to {Config.OUTPUT_DIR}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 15: Sample Predictions Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 15: SAMPLE PREDICTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def visualize_detections(model, images_dir, annotations_path, title, num_samples=4):\n",
    "    \"\"\"Visualize model detections on sample images.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with open(annotations_path, 'r') as f:\n",
    "        coco = json.load(f)\n",
    "    \n",
    "    img_to_anns = defaultdict(list)\n",
    "    for ann in coco['annotations']:\n",
    "        img_to_anns[ann['image_id']].append(ann)\n",
    "    \n",
    "    samples = random.sample(coco['images'], min(num_samples, len(coco['images'])))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(4*num_samples, 8))\n",
    "    \n",
    "    for i, img_info in enumerate(samples):\n",
    "        img_path = images_dir / img_info['file_name']\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Ground truth\n",
    "        gt_img = img_rgb.copy()\n",
    "        for ann in img_to_anns[img_info['id']]:\n",
    "            x, y, w, h = map(int, ann['bbox'])\n",
    "            cv2.rectangle(gt_img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        \n",
    "        axes[0, i].imshow(gt_img)\n",
    "        axes[0, i].set_title(f'GT ({len(img_to_anns[img_info[\"id\"]])})')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Predictions\n",
    "        img_tensor = torch.from_numpy(img_rgb).permute(2, 0, 1).float() / 255.0\n",
    "        img_tensor = img_tensor.to(Config.DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model([img_tensor])[0]\n",
    "        \n",
    "        pred_img = img_rgb.copy()\n",
    "        mask = (output['scores'] > Config.CONF_THRESHOLD) & (output['labels'] == 1)\n",
    "        boxes = output['boxes'][mask].cpu().numpy()\n",
    "        scores = output['scores'][mask].cpu().numpy()\n",
    "        \n",
    "        for box, score in zip(boxes, scores):\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            cv2.rectangle(pred_img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "            cv2.putText(pred_img, f'{score:.2f}', (x1, y1-5),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "        \n",
    "        axes[1, i].imshow(pred_img)\n",
    "        axes[1, i].set_title(f'Pred ({len(boxes)})')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Visualize Model B on clean and perturbed\n",
    "print(\"Model B predictions on CLEAN test images:\")\n",
    "fig1 = visualize_detections(model_B, IMAGES_DIR, TEST_ANN, \"Model B - Clean Test Set\")\n",
    "fig1.savefig(f\"{Config.OUTPUT_DIR}/model_B_clean_predictions.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nModel B predictions on PERTURBED test images:\")\n",
    "fig2 = visualize_detections(model_B, perturbed_dir, PERTURBED_TEST_ANN, \"Model B - Perturbed Test Set\")\n",
    "fig2.savefig(f\"{Config.OUTPUT_DIR}/model_B_perturbed_predictions.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 16: Save Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 16: SAVE EXPERIMENT SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "summary = {\n",
    "    'config': {\n",
    "        'image_size': Config.IMG_SIZE,\n",
    "        'num_epochs': Config.NUM_EPOCHS,\n",
    "        'batch_size': Config.BATCH_SIZE,\n",
    "        'learning_rate': Config.LR,\n",
    "        'iou_threshold': Config.IOU_THRESHOLD,\n",
    "        'conf_threshold': Config.CONF_THRESHOLD\n",
    "    },\n",
    "    'model_A_baseline': {\n",
    "        'training_history': history_A,\n",
    "        'test_clean': metrics_A_clean,\n",
    "        'test_perturbed': metrics_A_perturbed,\n",
    "        'robustness_drop_percent': drop_A\n",
    "    },\n",
    "    'model_B_augmented': {\n",
    "        'training_history': history_B,\n",
    "        'test_clean': metrics_B_clean,\n",
    "        'test_perturbed': metrics_B_perturbed,\n",
    "        'robustness_drop_percent': drop_B\n",
    "    },\n",
    "    'conclusion': {\n",
    "        'augmentation_improves_robustness': drop_B < drop_A,\n",
    "        'robustness_improvement_percent': drop_A - drop_B\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{Config.OUTPUT_DIR}/experiment_summary.json\", 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\"\"\n",
    "Saved outputs:\n",
    "  - Checkpoints: {Config.CHECKPOINT_DIR}/\n",
    "    - model_A_baseline_best.pth\n",
    "    - model_B_augmented_best.pth\n",
    "  - Visualizations: {Config.OUTPUT_DIR}/\n",
    "    - training_comparison.png\n",
    "    - metrics_comparison.png\n",
    "    - model_B_*_predictions.png\n",
    "  - Summary: {Config.OUTPUT_DIR}/experiment_summary.json\n",
    "\n",
    "Key Results:\n",
    "  - Model A (Baseline) F1 drop on perturbed: {drop_A:.1f}%\n",
    "  - Model B (Augmented) F1 drop on perturbed: {drop_B:.1f}%\n",
    "  - SAR augmentation improves robustness by: {drop_A - drop_B:.1f}%\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "### Key Fixes in This Version:\n",
    "1. **Box Format**: Proper conversion from COCO [x,y,w,h] to Faster R-CNN [x1,y1,x2,y2]\n",
    "2. **Evaluation**: Greedy matching sorted by confidence, proper FP/FN counting\n",
    "3. **Class Filtering**: Only count predictions with label=1 (person)\n",
    "4. **Debug Info**: TP/FP/FN printed each epoch for verification\n",
    "\n",
    "### Why Recall=1 Was Happening:\n",
    "- The original code may have had box format mismatches\n",
    "- Or the GT boxes were being compared incorrectly\n",
    "- This version properly handles all conversions\n",
    "\n",
    "### Swapping to YOLOv8:\n",
    "```python\n",
    "from ultralytics import YOLO\n",
    "model = YOLO('yolov8n.pt')\n",
    "model.train(data='data.yaml', epochs=6, imgsz=512, batch=4)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
